{
  "videoId": "-g1yKRo5XtY",
  "title": "How I code with AI right now",
  "url": "https://www.youtube.com/watch?v=-g1yKRo5XtY",
  "publishedAt": "1 day ago",
  "description": "The way I'm using AI has changed, a lot. And I think it's finally time to talk about it...\n\nThanks you Arcjet for sponsoring! Check them out at: https://soydev.link/arcjet\n\nSOURCES\nhttps://x.com/bm...",
  "thumbnailUrl": "https://i.ytimg.com/vi/-g1yKRo5XtY/hqdefault.jpg?sqp=-oaymwEcCNACELwBSFXyq4qpAw4IARUAAIhCGAFwAcABBg==&rs=AOn4CLD4IeAbL49yMi9ba005bpblAHGOiQ",
  "transcript": [
    {
      "text": "Remember that video I did about how I was blown away with Copilot all the way back in like 2023? It's been almost 2",
      "duration": 6240,
      "offset": "0"
    },
    {
      "text": "years since that video was published, and the way I use AI to write code has changed more than I ever would have imagined. I'm still on Cursor, but the",
      "duration": 6880,
      "offset": "6240"
    },
    {
      "text": "ways I use it have changed so fundamentally that I wanted to try and break down what my workflows look like",
      "duration": 5760,
      "offset": "13120"
    },
    {
      "text": "when I code with AI. I have been building a lot more recently, especially these last few days. And I think I've",
      "duration": 5840,
      "offset": "18880"
    },
    {
      "text": "really settled into a flow that I like enough to share with you guys. From how I use AI models for planning, execution,",
      "duration": 6639,
      "offset": "24720"
    },
    {
      "text": "how I do reviews, how I use work trees, how I've kind of changed how I use Git on a fundamental level. I'm really happy",
      "duration": 6081,
      "offset": "31359"
    },
    {
      "text": "with where I've landed and how I integrate AI into my workflows for my day-to-day coding tasks. It has made me",
      "duration": 6320,
      "offset": "37440"
    },
    {
      "text": "much more productive and overall having a lot more fun doing my engineering work as a result. I really want to emphasize",
      "duration": 5680,
      "offset": "43760"
    },
    {
      "text": "a few things before we dive in though. Thing one, this is not a Vibe coding tutorial. We're not here to recommend",
      "duration": 5920,
      "offset": "49440"
    },
    {
      "text": "ways to write code without understanding code. If you don't know how to code, go learn how to code. Come back here after.",
      "duration": 5519,
      "offset": "55360"
    },
    {
      "text": "This video is not for people who don't know how to write code yet. This is how to use these tools as an existing",
      "duration": 5041,
      "offset": "60879"
    },
    {
      "text": "productive software engineer to be more productive. The other thing that's important to note is that we'll be talking about a lot of companies that",
      "duration": 5600,
      "offset": "65920"
    },
    {
      "text": "I've invested money in because I like the space and when someone does something I enjoy, I try to throw money",
      "duration": 5840,
      "offset": "71520"
    },
    {
      "text": "at them because it's a good chance if I like it, they're going to find success. So, account for all sorts of biases",
      "duration": 5439,
      "offset": "77360"
    },
    {
      "text": "throughout this video. We're here to talk about the engineering work though, not about how I'm a paid shill for some",
      "duration": 5680,
      "offset": "82799"
    },
    {
      "text": "thing that has made my life way better and more productive. I really want to showcase how I use these things every day and the tricks and tips I've learned",
      "duration": 6640,
      "offset": "88479"
    },
    {
      "text": "from all the others around me. from how I work in my editor to how I handle code reviews to how I work on stuff with AI.",
      "duration": 7040,
      "offset": "95119"
    },
    {
      "text": "None of the tools we're talking about here paid me to make this video. In fact, I wasn't paid at all for this. Okay, I'm about to be paid by today's",
      "duration": 6721,
      "offset": "102159"
    },
    {
      "text": "sponsor. Quick question. If I was to spam your most expensive endpoint with thousands of requests, how much would it",
      "duration": 6160,
      "offset": "108880"
    },
    {
      "text": "cost you? If this was like 5 years ago, it would cost you almost nothing. But nowadays, with all these endpoints that",
      "duration": 5200,
      "offset": "115040"
    },
    {
      "text": "are backed by LLMs, a given request, just one request can cost you multiple dollars. If I spam you with hundreds or",
      "duration": 6800,
      "offset": "120240"
    },
    {
      "text": "thousands of them, how are you going to handle that? How much is it going to cost you? How easily can a malicious",
      "duration": 5600,
      "offset": "127040"
    },
    {
      "text": "actor screw your entire service by just hitting the right endpoint the right way? How reliable is the code and the",
      "duration": 5920,
      "offset": "132640"
    },
    {
      "text": "firewall and everything else you put together? Can you see it in code review? Does it work in the beta builds? Does it work in your test environments? How do",
      "duration": 6800,
      "offset": "138560"
    },
    {
      "text": "you actually know? I know because with recent projects I've been building with today's sponsor, ArcJet. These guys make",
      "duration": 5599,
      "offset": "145360"
    },
    {
      "text": "it so much easier to secure your services because they're not some crappy dashboard you have to go click all the",
      "duration": 5121,
      "offset": "150959"
    },
    {
      "text": "right buttons in and pray you did it right. They're just code. Here's how insanely complex it is to set up. You",
      "duration": 5920,
      "offset": "156080"
    },
    {
      "text": "install their package. You add your API key and then you copy paste this code block that is mostly comments that are",
      "duration": 5840,
      "offset": "162000"
    },
    {
      "text": "just commenting out things that you might want to use. So how do you actually use it? You go to your get",
      "duration": 5119,
      "offset": "167840"
    },
    {
      "text": "request. You have con decision equals await aj.protect. pass it the request and it tells you the decision and even",
      "duration": 6321,
      "offset": "172959"
    },
    {
      "text": "tells you the reason. But what happens if the user can access the thing but they shouldn't access it too much? You",
      "duration": 5360,
      "offset": "179280"
    },
    {
      "text": "might build your own crazy complex rate limiting service that's split across 15 things like we had to for T3 chat. And",
      "duration": 5920,
      "offset": "184640"
    },
    {
      "text": "if I was building T3 chat today, that's not how I would do it. I would use the token bucket from ArcJet because it's so",
      "duration": 5840,
      "offset": "190560"
    },
    {
      "text": "much simpler. Remember that rules array before? Now you can also define a token bucket in it. Mode characteristics. This",
      "duration": 6240,
      "offset": "196400"
    },
    {
      "text": "is how you're identifying the characteristic for this user and like separating them from others so that three different users aren't hitting the",
      "duration": 6160,
      "offset": "202640"
    },
    {
      "text": "same bucket. A refill rate, an interval, and a capacity. Couldn't be simpler. You have the user ID and you pass that to",
      "duration": 5999,
      "offset": "208800"
    },
    {
      "text": "AJ.protect. And you could even specify how many of a thing is requested because you might be managing how many tokens a",
      "duration": 6401,
      "offset": "214799"
    },
    {
      "text": "user can spend and different models cost different amounts. It's these things are annoying to do. They make it way easier. It's just code. No dashboards, no weird",
      "duration": 7119,
      "offset": "221200"
    },
    {
      "text": "config files, no YAML, no [ __ ] You just write TypeScript code in your TypeScript project and now you're safer.",
      "duration": 6081,
      "offset": "228319"
    },
    {
      "text": "If your service isn't secure, I don't blame you. Getting it right was way too hard to do before. It no longer is",
      "duration": 5440,
      "offset": "234400"
    },
    {
      "text": "thanks to ArcJet. Check them out now at soyv.link/archjet. On one hand, I'm tempted to go into an existing project and we probably will in",
      "duration": 6640,
      "offset": "239840"
    },
    {
      "text": "a bit, but I want to start with something new to show you guys from like the start how I use AI to scaffold,",
      "duration": 6560,
      "offset": "246480"
    },
    {
      "text": "plan, and build my projects. Obviously, AI stuff works really well for green field and small projects. You can make",
      "duration": 5759,
      "offset": "253040"
    },
    {
      "text": "it work great for big ones as well, especially if you use planning properly. But for now, we're going to start a new project to really showcase the strength",
      "duration": 6481,
      "offset": "258799"
    },
    {
      "text": "of these tools. And hopefully cursor won't break because I've had a rough time with curs last few days, working closely with them to get all these bugs",
      "duration": 5359,
      "offset": "265280"
    },
    {
      "text": "fixed. But we'll get there. So, I have an idea for a project that I want to build here with you guys. I am",
      "duration": 5921,
      "offset": "270639"
    },
    {
      "text": "interested in the quality of writing across various models and comparing the writing styles. I also had this random",
      "duration": 6320,
      "offset": "276560"
    },
    {
      "text": "idea of wouldn't it be cool if you had a model that's better at writing give feedback to another model about the",
      "duration": 6000,
      "offset": "282880"
    },
    {
      "text": "thing it wrote. So what I want to create is a tool that has one model write a draft for a prompt. Another model give",
      "duration": 6480,
      "offset": "288880"
    },
    {
      "text": "feedback and then the first model applies the feedback and you can compare these two resulting drafts. I think this",
      "duration": 6720,
      "offset": "295360"
    },
    {
      "text": "is going to be fun. First thing I recommend is that you actually init your projects yourself. I know the AI can run",
      "duration": 6240,
      "offset": "302080"
    },
    {
      "text": "commands and do all these things, but it's a lot easier to get it right if you control the initial environment. Using",
      "duration": 6719,
      "offset": "308320"
    },
    {
      "text": "stuff like create T3 app, better tstack, or even just a create next app or bunit",
      "duration": 5121,
      "offset": "315039"
    },
    {
      "text": "will help your tools start in a much better place. So that's what we're going to be doing here. Bunanet blank because",
      "duration": 6319,
      "offset": "320160"
    },
    {
      "text": "I don't want this to be a web app. And now we have all the files in here. Going to open cursor up with this project.",
      "duration": 6000,
      "offset": "326479"
    },
    {
      "text": "Right now we're in the editor view. We'll go back to the editor view in a bit. I'm going to use the command E hotkey to switch over to agents mode in",
      "duration": 6720,
      "offset": "332479"
    },
    {
      "text": "cursor. I have found myself genuinely really liking the agent mode. It's a",
      "duration": 5280,
      "offset": "339199"
    },
    {
      "text": "really nice way to plan things out and look at the code as a result of the work you're doing rather than squeezing this",
      "duration": 5841,
      "offset": "344479"
    },
    {
      "text": "chat interface into a traditional text editor. I find myself switching between the two a lot. My only wish and uh we'll",
      "duration": 7040,
      "offset": "350320"
    },
    {
      "text": "be talking a lot about this in the future cursor. I'm not giving them UX feedback yet because they haven't earned the right to it. There's too many bugs",
      "duration": 6000,
      "offset": "357360"
    },
    {
      "text": "they have to fix first. But one of the pieces of UX I would like for them to change is that the editor view has the",
      "duration": 5760,
      "offset": "363360"
    },
    {
      "text": "agent hidden by default. And when I switch to and from it, the agent only becomes visible in editor when I hit the",
      "duration": 5840,
      "offset": "369120"
    },
    {
      "text": "key. The same way the editor isn't visible in agent by default. We have a new project. Nothing happening here.",
      "duration": 5040,
      "offset": "374960"
    },
    {
      "text": "Opus is my model. Opus has been the model I'm using for most of my code recently. But I don't want to code just yet. I want to plan. So let's plan this",
      "duration": 7039,
      "offset": "380000"
    },
    {
      "text": "project out. I know a couple of the tools that I want to use for this. I could ask the model to set it up and I",
      "duration": 6880,
      "offset": "387039"
    },
    {
      "text": "will here for the sake of demoing, but I know I want to use the AISDK and that I want to use open router. So,",
      "duration": 5921,
      "offset": "393919"
    },
    {
      "text": "traditionally I would probably still go set those up manually because it's not that hard to do, but I'm not going to do that just yet. One thing I am going to",
      "duration": 6639,
      "offset": "399840"
    },
    {
      "text": "do though is set up some pieces for my workspace and cursor. It seems like bun and nit is now smart enough to specify",
      "duration": 6641,
      "offset": "406479"
    },
    {
      "text": "in cursor rules in this directory here to use bun instead of node vit npm and pnpm. That's a good thing that they're",
      "duration": 6799,
      "offset": "413120"
    },
    {
      "text": "adding. I am pumped that they are now ahead of the curve in telling your editor to not use things it shouldn't",
      "duration": 6560,
      "offset": "419919"
    },
    {
      "text": "be. But we're going to be using work trees. So I'm just going to copy this setup script I have here because this",
      "duration": 5761,
      "offset": "426479"
    },
    {
      "text": "setup script does two things. It uninstalls whenever a new work tree is created. And it also copies thev file",
      "duration": 7519,
      "offset": "432240"
    },
    {
      "text": "from the root to this new work tree. We'll talk about what work trees are and",
      "duration": 5521,
      "offset": "439759"
    },
    {
      "text": "why I like them so much in a bit. We're not there just yet. But I'm going to just paste this file in for us to get",
      "duration": 5199,
      "offset": "445280"
    },
    {
      "text": "started with. I'll make sure this repo is available publicly if you want to grab any of this code. Normally the work",
      "duration": 5041,
      "offset": "450479"
    },
    {
      "text": "tree configurator comes up as part of your setup when you run a work tree for the first time, but I just like having it by default. I might even make a new",
      "duration": 6959,
      "offset": "455520"
    },
    {
      "text": "template for like how I start projects with all my cursor stuff in the future. I don't use that much though. I'm relatively minimal on rules and tools",
      "duration": 6881,
      "offset": "462479"
    },
    {
      "text": "and all the other things that I add. I don't use any MCP servers or any stuff like that. Don't find it necessary. So",
      "duration": 5920,
      "offset": "469360"
    },
    {
      "text": "the order of events we're about to engage in, we're going to start with planning to make a plan for how we want to build and then we're going to do the",
      "duration": 6080,
      "offset": "475280"
    },
    {
      "text": "building. This needs to be a git repo. So I'm going to init quick. Now we have our initialized git repo. We want to",
      "duration": 7600,
      "offset": "481360"
    },
    {
      "text": "give enough context for what we want it to plan before we start executing the plan. I could type out all that context,",
      "duration": 6560,
      "offset": "488960"
    },
    {
      "text": "but honestly I find that I give more context when I use my voice than when I type. I really really like using",
      "duration": 6480,
      "offset": "495520"
    },
    {
      "text": "something like Whisper Flow, which is going to hopefully be a sponsor soon if all goes well. When I'm trying to do",
      "duration": 5840,
      "offset": "502000"
    },
    {
      "text": "this type of prompting, I could type it all out, but I'm not going to type as much as I'll talk. And sometimes I'll realize things as I'm talking, which is",
      "duration": 7600,
      "offset": "507840"
    },
    {
      "text": "nice. Let's tell it what I want. This project is going to be a demonstration of how different AI models handle",
      "duration": 6880,
      "offset": "515440"
    },
    {
      "text": "writing tasks. My goal is simple. Generate an essay based on a prompt the",
      "duration": 5040,
      "offset": "522320"
    },
    {
      "text": "user provides. have a different AI model review this essay and give feedback and",
      "duration": 5520,
      "offset": "527360"
    },
    {
      "text": "then hand that feedback back to the original model to update the essay accordingly. All three of these steps",
      "duration": 6079,
      "offset": "532880"
    },
    {
      "text": "should be saved as markdown files when the generation occurs so that I can easily review them in a text editor. Use",
      "duration": 6560,
      "offset": "538959"
    },
    {
      "text": "the AISDK by versel and open router in the open router provider for the AISDK",
      "duration": 5841,
      "offset": "545519"
    },
    {
      "text": "for the provisioning of the inference. Make sure everything you write is type safe. I think that's enough. This is a",
      "duration": 7120,
      "offset": "551360"
    },
    {
      "text": "decent enough starting point. I could do a bunch of bullet points and things, but I think this is fine. Opus seems to be a",
      "duration": 5520,
      "offset": "558480"
    },
    {
      "text": "good enough planning model, but honestly, I still have a soft spot for GPT 5.1 for planning. It's not the best",
      "duration": 6480,
      "offset": "564000"
    },
    {
      "text": "at tool calling reliably. I've honestly found that the models that call tools the best are most of the anthropic ones",
      "duration": 6560,
      "offset": "570480"
    },
    {
      "text": "as well as composer 1. We'll show you how I use all of those momentarily",
      "duration": 5040,
      "offset": "577040"
    },
    {
      "text": "because first we have to submit our planning. Now it's going to ask us some questions.",
      "duration": 5439,
      "offset": "582080"
    },
    {
      "text": "How do we want to use this essay review revision pipeline? Simple CLI script I",
      "duration": 5121,
      "offset": "587519"
    },
    {
      "text": "run manually that asks for a prompt then writes three markdown files. That sounds right to me. CLI tool that takes the",
      "duration": 5280,
      "offset": "592640"
    },
    {
      "text": "prompt as command line args or standard and writes the files. That could work. Definitely don't want HTTP and I definitely don't want a web UI. So we'll",
      "duration": 5840,
      "offset": "597920"
    },
    {
      "text": "do that for the two roles. SA writer versus reviewer. How do you want to choose models? Uh hardcode specific",
      "duration": 6079,
      "offset": "603760"
    },
    {
      "text": "route models for now. I like that plan. Continue.",
      "duration": 5120,
      "offset": "609839"
    },
    {
      "text": "And now it is creating a plan. Now this part is very important and might be hard for you vibe coding folks. See this",
      "duration": 7281,
      "offset": "614959"
    },
    {
      "text": "plan? I know this sounds crazy, like absurd, but hear me out. You should read",
      "duration": 5520,
      "offset": "622240"
    },
    {
      "text": "the plan. I know just like what you should do work when you're using AI. Isn't that the whole point? You could",
      "duration": 6560,
      "offset": "627760"
    },
    {
      "text": "ask another AI to do it for you, but you should read the plan. Sorry, the the",
      "duration": 5120,
      "offset": "634320"
    },
    {
      "text": "plan.PlMD plan.plan. That's how you know it's going to be good. So let's take a look",
      "duration": 5680,
      "offset": "639440"
    },
    {
      "text": "here. Implement a bun friendly TypeScript CLI that prompts the user for an essay topic. Uses model A overview",
      "duration": 5520,
      "offset": "645120"
    },
    {
      "text": "SDK to generate essay, model B to review the essay, produce feedback, and then model A again with the feedback to produce a revised essay. Saves all three",
      "duration": 7280,
      "offset": "650640"
    },
    {
      "text": "artifacts as markdown files on disk in a consistent location. High level design details. Keep using bun with index main",
      "duration": 6240,
      "offset": "657920"
    },
    {
      "text": "CLI AI client setup. Add AI open AI in the open router provider. Well, let's",
      "duration": 5760,
      "offset": "664160"
    },
    {
      "text": "give it some feedback. Why are you including the OpenAI package? We're",
      "duration": 6560,
      "offset": "669920"
    },
    {
      "text": "going to use Open router, not OpenAI.",
      "duration": 5039,
      "offset": "676480"
    },
    {
      "text": "Now, it will adjust that. You can keep reading. While we do that, configure a small AI client module or inline an",
      "duration": 5521,
      "offset": "681519"
    },
    {
      "text": "index ts a very small that wires the SDK to open router using an open router API key environment variable. Hardcode the",
      "duration": 6239,
      "offset": "687040"
    },
    {
      "text": "two model IDs, one for generation, one for review with clear cons name so you can easily change them later. Now, the",
      "duration": 5041,
      "offset": "693279"
    },
    {
      "text": "pipeline orchestration. Implement a run essay pipeline function. Reads the prompt from standard in or default if",
      "duration": 5840,
      "offset": "698320"
    },
    {
      "text": "standard in is empty. Calls the essay model with a system prompt plus a user prompt to generate the initial essay.",
      "duration": 5280,
      "offset": "704160"
    },
    {
      "text": "Call the review model with system instructions plus the essay contents to generate structured feedback. Call the",
      "duration": 5040,
      "offset": "709440"
    },
    {
      "text": "essay model again. Keep everything strongly typed. The markdown file output. Okay. Simple naming scheme. Runs",
      "duration": 6479,
      "offset": "714480"
    },
    {
      "text": "timestamp essay. Time stamp review time stamp revision. That looks fine. bun node FS APIs include basic front matter",
      "duration": 7841,
      "offset": "720959"
    },
    {
      "text": "or headings. All looks good to me. Implementation steps. It left the OpenAI",
      "duration": 6080,
      "offset": "728800"
    },
    {
      "text": "package here. More changes. Just going to write these all out once cuz a good chance we're going to need more. You",
      "duration": 6639,
      "offset": "734880"
    },
    {
      "text": "still have OpenAI in the setup depths section. AI client instantiate the SDK",
      "duration": 6961,
      "offset": "741519"
    },
    {
      "text": "with the open order provider. Exposes typed helpers like generate essay, review essay, and revise essay. pipeline",
      "duration": 6000,
      "offset": "748480"
    },
    {
      "text": "logic implement the run say pipeline interactively ask for prompts via standard in duh",
      "duration": 7320,
      "offset": "754480"
    },
    {
      "text": "this all looks good to me I'm going to just grab an env file quick because I already have one that has everything I",
      "duration": 6000,
      "offset": "763760"
    },
    {
      "text": "need for this make sure the runs directory is included in the git ignore",
      "duration": 8319,
      "offset": "769760"
    },
    {
      "text": "this all looks good so far I could just hit the build button here or here but I'm going to do things a a little",
      "duration": 5921,
      "offset": "778079"
    },
    {
      "text": "differently cuz I have separate preferences. Here's where we're going to start using work trees. If you're not",
      "duration": 5120,
      "offset": "784000"
    },
    {
      "text": "familiar with work trees, it's a way to take a git branch for a project and put it temporarily in a different directory.",
      "duration": 6080,
      "offset": "789120"
    },
    {
      "text": "So, you can have multiple branches on your machine at the same time. This is really useful for when you're doing different work in parallel. It's",
      "duration": 6319,
      "offset": "795200"
    },
    {
      "text": "incredibly useful if you're working on a task. You notice something else you want to touch, but you don't want to clog up",
      "duration": 5521,
      "offset": "801519"
    },
    {
      "text": "the PR you're currently working on. So you can spin up another agent with a work tree that will be given a new",
      "duration": 5520,
      "offset": "807040"
    },
    {
      "text": "directory just for it to work in. And once you're done, you can choose to merge in those changes. It's somewhere",
      "duration": 5440,
      "offset": "812560"
    },
    {
      "text": "between a background agent and traditional like the agent controlling your editor. There's like the three",
      "duration": 5360,
      "offset": "818000"
    },
    {
      "text": "layers of how deep these agents go, like I guess how far they are from the work you're doing. Traditionally when you",
      "duration": 6159,
      "offset": "823360"
    },
    {
      "text": "like command I in your editor and have the little sidebar that is using the",
      "duration": 5041,
      "offset": "829519"
    },
    {
      "text": "agent to control the editor and change code in your directory direct like right there. There's backard agents where it",
      "duration": 5920,
      "offset": "834560"
    },
    {
      "text": "clones your repo in the cloud and does edits in the cloud that you can then merge in or make a PR for when you want",
      "duration": 5520,
      "offset": "840480"
    },
    {
      "text": "and then work trees which are somewhere between the two where they run on your machine but in parallel to your other stuff. They're also really useful",
      "duration": 6160,
      "offset": "846000"
    },
    {
      "text": "because you can use it to compare different models. So here I have composer, opus, sonnet, and gemini all selected. And if I wanted, I could",
      "duration": 6799,
      "offset": "852160"
    },
    {
      "text": "change the number of each of them being used. I could have three instances of composer running over this task at the",
      "duration": 5761,
      "offset": "858959"
    },
    {
      "text": "same time. I'm going to use just one for composer, opus, and screw it, Gemini 3",
      "duration": 5280,
      "offset": "864720"
    },
    {
      "text": "pro. People like that model, right? Cool. These are the models I'll use. Obviously, Composer 1 is the really fast, relatively cheap one that was",
      "duration": 6000,
      "offset": "870000"
    },
    {
      "text": "built by our friends at Cursor. I do like it a lot for these types of things. Opus will be fun to see how it behaves",
      "duration": 5040,
      "offset": "876000"
    },
    {
      "text": "here, and Gemini 3 Pro will be as well. So if it turns out the DOM model is not good enough for this, I could use one of",
      "duration": 5520,
      "offset": "881040"
    },
    {
      "text": "those instead. And here I can reference the plan by just adding it. Now it's",
      "duration": 5200,
      "offset": "886560"
    },
    {
      "text": "here. I have the set for work trees based off of main. So this is the branch it's going to use when it creates the work trees. And I'll tell it implement",
      "duration": 7680,
      "offset": "891760"
    },
    {
      "text": "the project. Send. And now we have three different instances running in their own",
      "duration": 7360,
      "offset": "899440"
    },
    {
      "text": "paths in their own branches doing this implementation. And if we want, I can go",
      "duration": 5279,
      "offset": "906800"
    },
    {
      "text": "to any one of these in my terminal. I bullied them into adding this little button here, copy workree path. Previously, they only had open terminal",
      "duration": 6320,
      "offset": "912079"
    },
    {
      "text": "in workree, which would open the terminal in the VS Code, well, cursor instance, which I hated. But now, copy",
      "duration": 7041,
      "offset": "918399"
    },
    {
      "text": "workree path. Go into your terminal, make a new tab, cd, and here is the path. Notice it's inturs. That's where",
      "duration": 6639,
      "offset": "925440"
    },
    {
      "text": "they dump all of these work trees by default. And it has all the files for this branch. This is also why I",
      "duration": 6401,
      "offset": "932079"
    },
    {
      "text": "introduced the work trees JSON file because when a new work tree is created, I want it to run bun install and also",
      "duration": 6159,
      "offset": "938480"
    },
    {
      "text": "copy over the environment variables. So if we go back here, you'll see if we do ls- aenv is here, which it wouldn't have",
      "duration": 7440,
      "offset": "944639"
    },
    {
      "text": "been otherwise because it wouldn't copy the git ignore files otherwise. Let's see how the agent is doing. Oh no, can",
      "duration": 6641,
      "offset": "952079"
    },
    {
      "text": "they not find the cursor? I I'm going to send a very rudely worded",
      "duration": 8239,
      "offset": "958720"
    },
    {
      "text": "message to my shared Slack with cursor. One moment, please. Rude DM sent.",
      "duration": 5521,
      "offset": "966959"
    },
    {
      "text": "Let's go back here. Let me choose how I want to deal with this. There are options. One of the easiest by far is to",
      "duration": 7039,
      "offset": "972480"
    },
    {
      "text": "just copy the plan file and put it in your editor. Ideally, by the time you're doing this, they will have this [ __ ]",
      "duration": 5760,
      "offset": "979519"
    },
    {
      "text": "fixed. But right now, it is entirely [ __ ] broken. So, I'm doing my best. I'll put this in dot cursor. Let's try",
      "duration": 7360,
      "offset": "985279"
    },
    {
      "text": "again. Work tree, same models at planmd",
      "duration": 6000,
      "offset": "992639"
    },
    {
      "text": "implement the plan and build out the project. And",
      "duration": 6401,
      "offset": "998639"
    },
    {
      "text": "this time it can access the plan because I put the file in the repo and it will get copied over and it created the",
      "duration": 6000,
      "offset": "1005040"
    },
    {
      "text": "to-dos. Now we have all three of these models going and we can see just how much composer flies. I have to allow it",
      "duration": 7280,
      "offset": "1011040"
    },
    {
      "text": "to run commands. Except Except.",
      "duration": 5600,
      "offset": "1018320"
    },
    {
      "text": "Yeah, Composer flies. It's not the smartest, but it's pretty dang good. God, it goes fast. It's almost done, and",
      "duration": 6720,
      "offset": "1023920"
    },
    {
      "text": "the rest are like one to three steps in, and none of these are even Gemini, like the slow model.",
      "duration": 6720,
      "offset": "1030640"
    },
    {
      "text": "Obviously, you don't need to run this with three different models at the same time. It's a flow I like because I like to compare the results and learn more",
      "duration": 6240,
      "offset": "1037360"
    },
    {
      "text": "about the models whenever I can. A lot of this should be taken as a learning opportunity and yeah, I like learning.",
      "duration": 6000,
      "offset": "1043600"
    },
    {
      "text": "So, let's go hop into this directory and see how that goes with the composer implementation. You can just read it",
      "duration": 6319,
      "offset": "1049600"
    },
    {
      "text": "right here if you want using the review tab, which I've actually grown to really, really like. The review tab is awesome because it groups all the files",
      "duration": 6401,
      "offset": "1055919"
    },
    {
      "text": "that were changed into one single view that you can scroll through, which is super handy. So, let's just read through",
      "duration": 5520,
      "offset": "1062320"
    },
    {
      "text": "it first and see. Has a run directory interface for the pipeline output. Uh",
      "duration": 6000,
      "offset": "1067840"
    },
    {
      "text": "async ensure runs directory that checks if the directory exists. Get timestamp string now is new date. Cool. Write",
      "duration": 7440,
      "offset": "1073840"
    },
    {
      "text": "pipeline outputs. We have the index. So prompt for topic.",
      "duration": 5840,
      "offset": "1081280"
    },
    {
      "text": "Enter your essay topic. You write the topic essay results await generate essay",
      "duration": 5280,
      "offset": "1087120"
    },
    {
      "text": "topic. So this is when it goes to generate the essay. Then we have the review",
      "duration": 5040,
      "offset": "1092400"
    },
    {
      "text": "revision results. Revise essay. Write pipeline.",
      "duration": 5520,
      "offset": "1097440"
    },
    {
      "text": "Cool. And we have in here all of those functions. Results a way generate text review",
      "duration": 6800,
      "offset": "1102960"
    },
    {
      "text": "model. You're an expert writing tutor and editor. Review the essay provided",
      "duration": 5120,
      "offset": "1109760"
    },
    {
      "text": "and give constructive specific feedback on areas such as structure, clarity, argumentation, style, and areas for",
      "duration": 6880,
      "offset": "1114880"
    },
    {
      "text": "improvement. Be thorough but encouraging. Please review the following essay and provide detailed feedback. And",
      "duration": 5840,
      "offset": "1121760"
    },
    {
      "text": "then revise essay as the differences in prompt. You're an expert essay writer. Revise the provided essay based on the",
      "duration": 6400,
      "offset": "1127600"
    },
    {
      "text": "feedback given while maintaining the core message topic and improving the areas identified. Cool.",
      "duration": 7760,
      "offset": "1134000"
    },
    {
      "text": "This looks like a good starting point for me. Let's give it a shot. Copy work",
      "duration": 5440,
      "offset": "1141760"
    },
    {
      "text": "tree cd to the right place. Fun run index.ts",
      "duration": 5200,
      "offset": "1147200"
    },
    {
      "text": "essay topic. Let's come up with a good one. Write a thorough breakdown of why",
      "duration": 5360,
      "offset": "1152400"
    },
    {
      "text": "explicit return types in Typescript might not be a good idea even though they are in other languages. Now it's",
      "duration": 6400,
      "offset": "1157760"
    },
    {
      "text": "generating the initial essay. I forgot to specify models. What models did it pick? I'm scared.",
      "duration": 6639,
      "offset": "1164160"
    },
    {
      "text": "Used for mini and 3.5 sonnet. That's great.",
      "duration": 5201,
      "offset": "1170799"
    },
    {
      "text": "We can go through and change those momentarily. Let's go grab the slugs that I want. Open router.ai.",
      "duration": 7039,
      "offset": "1176000"
    },
    {
      "text": "I want Opus 4.5",
      "duration": 6281,
      "offset": "1183039"
    },
    {
      "text": "the essay model and I want Kimmy K2 thinking",
      "duration": 6560,
      "offset": "1189520"
    },
    {
      "text": "for this one. Let's take a look at what it did in the interim. I'll often open up another cursor instance inside of the",
      "duration": 8640,
      "offset": "1196080"
    },
    {
      "text": "thing that I just created for the work tree because it's nice to be able to look at. So here's the essay case",
      "duration": 6240,
      "offset": "1204720"
    },
    {
      "text": "against explicit return types in Typescript. In the blossoming realm of web development, TypeScript has secured",
      "duration": 5040,
      "offset": "1210960"
    },
    {
      "text": "its position as a powerful supererset of JS. Yeah, this is so 40 it hurts. So",
      "duration": 6640,
      "offset": "1216000"
    },
    {
      "text": "let's try that again. Copy the same prompt. And if you guys were curious how",
      "duration": 6159,
      "offset": "1222640"
    },
    {
      "text": "fast the cursor team moves, I just got a message from them. They'll have a new build ready for me with the fixes I",
      "duration": 6481,
      "offset": "1228799"
    },
    {
      "text": "requested in 45 minutes. Kind of nuts. That team works hard. Essay was",
      "duration": 5040,
      "offset": "1235280"
    },
    {
      "text": "generated. Now it's reviewing. Revising essay based on feedback. Pipeline completed. Let's take a look. We have",
      "duration": 6640,
      "offset": "1240320"
    },
    {
      "text": "our new timestamp for the new run we just did.",
      "duration": 5199,
      "offset": "1246960"
    },
    {
      "text": "This one was written with Opus 4.5. The case is explicit return to TypeScript. When type inference shines already so",
      "duration": 6801,
      "offset": "1252159"
    },
    {
      "text": "much better than the 40 version. TypeScript has transformed JavaScript development by introducing static",
      "duration": 6160,
      "offset": "1258960"
    },
    {
      "text": "typings to a dynamically typed language. Among the many decisions typeser developers face is whether to explicitly",
      "duration": 6559,
      "offset": "1265120"
    },
    {
      "text": "annotate return types on functions or to allow the compiler to infer them. While languages like Java, C#, and Rust",
      "duration": 6641,
      "offset": "1271679"
    },
    {
      "text": "benefit greatly from explicit return type declarations, TypeScript occupies a unique position where mandatory explicit",
      "duration": 6000,
      "offset": "1278320"
    },
    {
      "text": "return types can actually work against the developer. This explores why TypeScript's type inference for the return types is often preferable to",
      "duration": 7040,
      "offset": "1284320"
    },
    {
      "text": "explicit annotations despite conventional wisdom from other type languages. Little verbose, not bad, though. Let's",
      "duration": 8080,
      "offset": "1291360"
    },
    {
      "text": "see what the review looks like. I'm curious. This is an excellent, well-argued essay on nuance types or topic. You struck a good balance between",
      "duration": 5680,
      "offset": "1299440"
    },
    {
      "text": "technical depth and readability. Your willingness to address counter arguments strengthens your position consistently. Let me offer detailed feedback to help",
      "duration": 5280,
      "offset": "1305120"
    },
    {
      "text": "elevate it further. The writing is clean and authoritative, though there are opportunities to sharpen certain arguments and vary your pros rhythm.",
      "duration": 7200,
      "offset": "1310400"
    },
    {
      "text": "Some section transitions feel abrupt. For example, between sections two and three, you could add a bridge sentence like, \"However, this power comes with a",
      "duration": 6160,
      "offset": "1317600"
    },
    {
      "text": "hidden cost when developers choose to override it.\" So that's between sections two and three.",
      "duration": 7399,
      "offset": "1323760"
    },
    {
      "text": "Yeah, it's they can also improve ID responsiveness when working with complex generic functions and then immediately to a different unrelated topic. Yeah,",
      "duration": 6561,
      "offset": "1333679"
    },
    {
      "text": "that is pretty sudden. I see why they would call that out. Let's see how the revised version came",
      "duration": 7360,
      "offset": "1340240"
    },
    {
      "text": "out. already like it more. Starting with an actual like pointed story, the first thing many devs do when learning",
      "duration": 5600,
      "offset": "1347600"
    },
    {
      "text": "TypeScript is annotate every function's return type. Second thing they do is spend hours maintaining those annotations after refactoring. This",
      "duration": 6000,
      "offset": "1353200"
    },
    {
      "text": "pattern reveals a fundamental tension in Typescript development. While languages like Java, C#, and Rust benefit greatly from explicit return type declarations,",
      "duration": 6719,
      "offset": "1359200"
    },
    {
      "text": "TypeScript occupies a unique position where mandatory explicit return types can actually work against the developer.",
      "duration": 6240,
      "offset": "1365919"
    },
    {
      "text": "Already so much better. Let's see how the transitions are now.",
      "duration": 6640,
      "offset": "1372159"
    },
    {
      "text": "Thanks. So the return type has here creates redundancy and maintenance overhead without providing additional",
      "duration": 5441,
      "offset": "1378799"
    },
    {
      "text": "safety. First off, I like this project. This is actually fun and useful to look into.",
      "duration": 5760,
      "offset": "1384240"
    },
    {
      "text": "And this is one of my favorite things to do with AI is when I have a theory, go code out that theory and experiment with",
      "duration": 6159,
      "offset": "1390000"
    },
    {
      "text": "it a bit. And now we have an experiment that's actually kind of cool and useful.",
      "duration": 5601,
      "offset": "1396159"
    },
    {
      "text": "I would love to do some type of like arena project where these can compete similar to like LM arena, but my initial",
      "duration": 7520,
      "offset": "1401760"
    },
    {
      "text": "thought is it'd be cool to have AI look at the two essays and say which one it",
      "duration": 5040,
      "offset": "1409280"
    },
    {
      "text": "thinks is better. I hope I've established that a dumb fast model is good enough for this once you have a",
      "duration": 5440,
      "offset": "1414320"
    },
    {
      "text": "good plan because the composer run was a lot faster and probably also a lot cheaper than both the Opus and Gemini 3",
      "duration": 5840,
      "offset": "1419760"
    },
    {
      "text": "runs. We can take a look at the Gemini code, see if it's any better here.",
      "duration": 5558,
      "offset": "1425600"
    },
    {
      "text": "I like the essay system prompt better. So, I think I'm just going to merge this. Yeah, I like this code more.",
      "duration": 6880,
      "offset": "1433200"
    },
    {
      "text": "The one catch with a project like this where we haven't done anything with it yet is that doesn't have code to",
      "duration": 6959,
      "offset": "1440080"
    },
    {
      "text": "reference for how it should format things. Like, I don't love the string formatting it's doing here. But, this is",
      "duration": 5041,
      "offset": "1447039"
    },
    {
      "text": "totally workable. I do want to make the string formatting less annoying, though. So, I'm going to",
      "duration": 5280,
      "offset": "1452080"
    },
    {
      "text": "ask it to do that. Can we use dent or something similar to",
      "duration": 7360,
      "offset": "1457360"
    },
    {
      "text": "make the string formatting less gross? And again, this is where being a real",
      "duration": 5360,
      "offset": "1464720"
    },
    {
      "text": "engineer is useful. I'm seeing formatting stuff that is obnoxious. I know a path to fix it. So, I'm",
      "duration": 5280,
      "offset": "1470080"
    },
    {
      "text": "recommending the path to fix it. And it's really cool. You can do these in the separate work trees because these",
      "duration": 6880,
      "offset": "1475360"
    },
    {
      "text": "are separate projects effectively. Let me switch over to this version and give",
      "duration": 5600,
      "offset": "1482240"
    },
    {
      "text": "it a shot. Copy my prompt quick. I'm just going to throw this in an empty thing here so I have it quick. Uh not",
      "duration": 8160,
      "offset": "1487840"
    },
    {
      "text": "there. Command option S. Here we are.",
      "duration": 5039,
      "offset": "1496000"
    },
    {
      "text": "Um I'll just put it here. Test prompt MD. Paste. Well, now I have",
      "duration": 8240,
      "offset": "1501039"
    },
    {
      "text": "that to grab whenever I want it. Bun run index. Oh, I forgot I was scrolled up. Bun run index. paste the topic. Oh, I",
      "duration": 7121,
      "offset": "1509279"
    },
    {
      "text": "forgot to choose the different models. Um, let me switch the models over quick, too. This is the one annoying thing",
      "duration": 5360,
      "offset": "1516400"
    },
    {
      "text": "about hopping between different things is if you make changes in one and you didn't make them on the other, you need",
      "duration": 5120,
      "offset": "1521760"
    },
    {
      "text": "to go back and handle that. The easiest thing I can do here rather than go find the specific code is just yoink",
      "duration": 8640,
      "offset": "1526880"
    },
    {
      "text": "these. Here's the models I want to use.",
      "duration": 6880,
      "offset": "1535520"
    },
    {
      "text": "Command shiftV for pasting without it linking to a file because I don't want to link from one work tree to the other. I just want the text content.",
      "duration": 6960,
      "offset": "1542400"
    },
    {
      "text": "Cool. Now that is done. Go back to the editor to copy my test prompt again.",
      "duration": 6000,
      "offset": "1549360"
    },
    {
      "text": "Paste. Send. Open this here so that I could take a look at the results. Oh, I",
      "duration": 5199,
      "offset": "1555360"
    },
    {
      "text": "think it hit one of the slow Kimmy K2 providers. What the hell did it do with all this",
      "duration": 5201,
      "offset": "1560559"
    },
    {
      "text": "empty output? That's annoying. I'll ask it to fix that. But let's take a look at the runs. Here's the original and",
      "duration": 7039,
      "offset": "1565760"
    },
    {
      "text": "screwed up the formatting. Man, I hate that I have to say this, but I think I'm happier with the version that was",
      "duration": 6000,
      "offset": "1572799"
    },
    {
      "text": "created by Composer than Opus cuz like the this is [ __ ] up the formatting a",
      "duration": 5601,
      "offset": "1578799"
    },
    {
      "text": "ton. Even here,",
      "duration": 4040,
      "offset": "1584400"
    },
    {
      "text": "um, I'll tell it that we'll give it one last chance to fix things. The indentation",
      "duration": 7681,
      "offset": "1592559"
    },
    {
      "text": "in the markdown files is all wonky. I",
      "duration": 5280,
      "offset": "1600240"
    },
    {
      "text": "screenshotted some examples. Please fix it. Let that fix it. But honestly, I'm",
      "duration": 5360,
      "offset": "1605520"
    },
    {
      "text": "leaning towards the composer version unless the quality of the essay is significantly better due to the system prompt difference. Yeah, I don't love",
      "duration": 7279,
      "offset": "1610880"
    },
    {
      "text": "the difference here. I think I'm going to stick with the version that I got from Composer. Honestly, it might have",
      "duration": 6561,
      "offset": "1618159"
    },
    {
      "text": "been a mistake. Honestly, now I think about it because of how we're writing this all out. That's annoying. String formatting is the most annoying thing in",
      "duration": 5199,
      "offset": "1624720"
    },
    {
      "text": "the [ __ ] world and no AI is smart enough to fix all that sadly. I'm going to rock the composer version. So now we",
      "duration": 7441,
      "offset": "1629919"
    },
    {
      "text": "can actually review it. I know I've read the code. I'm pretty happy with it. So I don't care too too much. But we're going",
      "duration": 5120,
      "offset": "1637360"
    },
    {
      "text": "to take a quick look. One last check.",
      "duration": 5400,
      "offset": "1642480"
    },
    {
      "text": "Yeah, I'm happy with it. We'll accept it. So now that I've accepted this, it just pulled all those changes in to the",
      "duration": 6320,
      "offset": "1648400"
    },
    {
      "text": "main branch in my main project folder here. So if I get diff, you'll see all",
      "duration": 6079,
      "offset": "1654720"
    },
    {
      "text": "those changes are now here. I'm going to do one last thing here that I think is",
      "duration": 5441,
      "offset": "1660799"
    },
    {
      "text": "almost always best practice. We're going to create a GitHub repo. I don't have the GitHub CLI installed. I have a lot",
      "duration": 6799,
      "offset": "1666240"
    },
    {
      "text": "of feelings about the GitHub CLI, but I'll try to use it at least.",
      "duration": 5520,
      "offset": "1673039"
    },
    {
      "text": "Push existing local repo. It is this one. Autodraftify.",
      "duration": 5120,
      "offset": "1678559"
    },
    {
      "text": "What do you mean it's not authenticated? You have my SSH key, [ __ ] Sorry. GH off login. Greatest piece of software.",
      "duration": 9041,
      "offset": "1683679"
    },
    {
      "text": "Oh, GitHub CLI. It doesn't appear to be smart enough to be done now that I did",
      "duration": 5920,
      "offset": "1692720"
    },
    {
      "text": "that. The GitHub CLI is legitimately one of the worst pieces of software I've ever [ __ ] used.",
      "duration": 5840,
      "offset": "1698640"
    },
    {
      "text": "Holy [ __ ] it's so bad. Apparently, you can't click the link. You have to press",
      "duration": 6400,
      "offset": "1704480"
    },
    {
      "text": "enter. So, here I can click that link, but if I do that, it won't work. I have to press",
      "duration": 7039,
      "offset": "1710880"
    },
    {
      "text": "enter and go through this way instead.",
      "duration": 4441,
      "offset": "1717919"
    },
    {
      "text": "I I made a video a while back about how the GitHub CLI is the most cursed pile of [ __ ] ever, and I got flamed for it in",
      "duration": 6561,
      "offset": "1725919"
    },
    {
      "text": "the first hour, so I took it down cuz I felt bad. I stand by every word. This is a tragedy. Not a piece of software. It",
      "duration": 6480,
      "offset": "1732480"
    },
    {
      "text": "has never done what it should ever once. Let let's let's do a quick poll in chat. Not a traditional poll. Just like we're",
      "duration": 6079,
      "offset": "1738960"
    },
    {
      "text": "going to have field drop numbers. How many flags or input fields do you think",
      "duration": 5841,
      "offset": "1745039"
    },
    {
      "text": "I'm going to have to fill out when I try to push this repo up using the CLI? How",
      "duration": 5360,
      "offset": "1750880"
    },
    {
      "text": "many steps do you think it'll take? And bonus points. how many of them are going to be blocking on a network request",
      "duration": 5840,
      "offset": "1756240"
    },
    {
      "text": "where I have to sit there and wait for the network request to go back and forth and specify both. So say",
      "duration": 7040,
      "offset": "1762080"
    },
    {
      "text": "uh inputs colon number blocking colon other number seven and two 9 and five",
      "duration": 7840,
      "offset": "1769120"
    },
    {
      "text": "five steps two blocks 6 plus two network y'all ready for this? So first one I",
      "duration": 5920,
      "offset": "1776960"
    },
    {
      "text": "have to do push existing repo I'm in a repo that's never been pushed that has no origin. You should be able to infer",
      "duration": 5279,
      "offset": "1782880"
    },
    {
      "text": "that path of local repo. I'm in the repo. Cool. It got the name right from the folder. It did one thing right so",
      "duration": 6081,
      "offset": "1788159"
    },
    {
      "text": "far. That was the first network block. It just network blocked there. Pick the project like the repository owner.",
      "duration": 6319,
      "offset": "1794240"
    },
    {
      "text": "That's the first one that made sense. Description. Why the [ __ ] do I need to write a description right now? Who",
      "duration": 5600,
      "offset": "1800559"
    },
    {
      "text": "writes a description before they put up the repo? When you go click the create button on the website, it doesn't make you put in a description. Skip. Polic",
      "duration": 7441,
      "offset": "1806159"
    },
    {
      "text": "internal. Another one that actually makes some sense. Cool. Cool. So, we've had two that make sense so far. That was",
      "duration": 5040,
      "offset": "1813600"
    },
    {
      "text": "a network block. Add a remote. No, I already told you I want the local repo. Why would I not want the remote? In what",
      "duration": 7680,
      "offset": "1818640"
    },
    {
      "text": "world would I say no here? Why would I ever? What should the new remote be",
      "duration": 5200,
      "offset": "1826320"
    },
    {
      "text": "called? I don't know. What do you think it should be called, GitHub? It's like",
      "duration": 5120,
      "offset": "1831520"
    },
    {
      "text": "every single flag on the API for this is exposed over the UI. Here we're at 1 2 3",
      "duration": 6159,
      "offset": "1836640"
    },
    {
      "text": "4 5 6 7 8 nine questions so far. Would I like to push the commits to the current",
      "duration": 6480,
      "offset": "1842799"
    },
    {
      "text": "branch to origin? Yeah. Step 11. 11 steps and three network blocks to push a",
      "duration": 6880,
      "offset": "1849279"
    },
    {
      "text": "repo up to GitHub. What the [ __ ] Anyways, pro tip on this. Actually, I'll",
      "duration": 5601,
      "offset": "1856159"
    },
    {
      "text": "show you in a second. Um, now this is pushed. get checkout-b Theo slash uh",
      "duration": 7279,
      "offset": "1861760"
    },
    {
      "text": "I'll just call this composer version get add-p I like -p a lot when I'm doing git",
      "duration": 5760,
      "offset": "1869039"
    },
    {
      "text": "additions because it's like a way to mini review changes as you go it doesn't add new files though so I'm just going",
      "duration": 5441,
      "offset": "1874799"
    },
    {
      "text": "to do dash a for that d-m composer implementation of autodraftify",
      "duration": 7520,
      "offset": "1880240"
    },
    {
      "text": "and now I'm going to push I already have my git config locally so if I push on a",
      "duration": 5120,
      "offset": "1887760"
    },
    {
      "text": "branch that isn't there yet it'll create the origin makes my life a lot easier. So, it created the remote because it",
      "duration": 6480,
      "offset": "1892880"
    },
    {
      "text": "didn't exist yet. I can click this link to autocreate, but GHPR create seems",
      "duration": 6319,
      "offset": "1899360"
    },
    {
      "text": "like a great idea, right? Except it then asks another 15 [ __ ] questions. And this is the best part. If the names of",
      "duration": 6801,
      "offset": "1905679"
    },
    {
      "text": "your commits are too long, it'll make a mal formatted URL that doesn't work. So, the way I make a PR, I'll show you. I",
      "duration": 6880,
      "offset": "1912480"
    },
    {
      "text": "use Lazy Git. I press three. I press O. literally 100 times faster. It's so much",
      "duration": 6480,
      "offset": "1919360"
    },
    {
      "text": "better. It's so much better. I I I don't even use any of the features in Lazy Git. I literally just use it as a way to",
      "duration": 5600,
      "offset": "1925840"
    },
    {
      "text": "open a PR faster. LG30. It's like embedded in my brain now. So, now we have this PR. Let's create it. And now",
      "duration": 7520,
      "offset": "1931440"
    },
    {
      "text": "for the next thing I like to use AI for. Reviews. There's a lot of options for",
      "duration": 5599,
      "offset": "1938960"
    },
    {
      "text": "this. And honestly, I don't care which you use. Two of them sponsor me. Most of them are good. I like Code Rabbit and I",
      "duration": 6561,
      "offset": "1944559"
    },
    {
      "text": "really like Reptile, too. They're both great options. All of them are fine, though. Grapile is the easiest for me to",
      "duration": 5600,
      "offset": "1951120"
    },
    {
      "text": "quickly set up in a new open source repo. So, I'm going to go set up grapile really quick. So, here I have grap tiles dashboard settings enabled. Let's add",
      "duration": 10559,
      "offset": "1956720"
    },
    {
      "text": "new one. Enable. I don't why there's two enable buttons. What are you guys doing?",
      "duration": 5841,
      "offset": "1967279"
    },
    {
      "text": "Someone was vibe coding a little too close to the sun. One sec to send that to them. Well, now we have this enabled",
      "duration": 7200,
      "offset": "1973120"
    },
    {
      "text": "on autodraftify. It's going to index. It shouldn't take too long to index because there's almost no code in there.",
      "duration": 6640,
      "offset": "1980320"
    },
    {
      "text": "But now that we've done that, I can add grapile review, please. And see that",
      "duration": 5680,
      "offset": "1986960"
    },
    {
      "text": "eyes emoji. I love this. This is how you know Grapile is doing its job. It is starting to do its review. So, in a",
      "duration": 5120,
      "offset": "1992640"
    },
    {
      "text": "little bit, Graile will have it review of this code. I'm not going to wait for that just yet, though. It is really nice",
      "duration": 5120,
      "offset": "1997760"
    },
    {
      "text": "to have another AI, another tool like this reviewing the changes just to do a quick pass on it. There are actually",
      "duration": 6240,
      "offset": "2002880"
    },
    {
      "text": "features in cursor to have an AI do the review in the editor. I don't like them.",
      "duration": 5600,
      "offset": "2009120"
    },
    {
      "text": "I like doing code reviews on a code review platform like Graphite or in this case, GitHub using Grapile. Lots of",
      "duration": 5760,
      "offset": "2014720"
    },
    {
      "text": "options. I don't care which. But I do really like PRs as ways to group the work you did and have another tool take",
      "duration": 6960,
      "offset": "2020480"
    },
    {
      "text": "one last look before you hit merge. To show you an example of how this looks, I'll show you some PRs I have open on",
      "duration": 6320,
      "offset": "2027440"
    },
    {
      "text": "SnitchBench right now. Here",
      "duration": 5200,
      "offset": "2033760"
    },
    {
      "text": "is a couple of the ASDKV5 migrations. This is one of my favorite tests for new models and Opus oneshotted it and did a",
      "duration": 6719,
      "offset": "2038960"
    },
    {
      "text": "pretty good job. Let's take a look at the gravile review here. Confidence score is two out of five. Pay close",
      "duration": 6160,
      "offset": "2045679"
    },
    {
      "text": "attention to index. The tool result extraction logic needs immediate correction before merging. Very interesting. Apparently, ASKV step the",
      "duration": 7760,
      "offset": "2051839"
    },
    {
      "text": "tool results returns objects with a result property, not output. This will cause the tool results to be undefined.",
      "duration": 5681,
      "offset": "2059599"
    },
    {
      "text": "I can commit the suggestion directly from here or copy this prompt, paste it into my editor and have it make the",
      "duration": 5598,
      "offset": "2065280"
    },
    {
      "text": "change there. That's a very good catch though. I'm very thankful that I ran this. See how useful these tools are.",
      "duration": 5921,
      "offset": "2070879"
    },
    {
      "text": "You get the idea. Very nice to have those types of things. But I've accepted this. I'm happy with the state of",
      "duration": 5200,
      "offset": "2076800"
    },
    {
      "text": "things. Let's keep grinding. So, all the tasks we've done so far have been relatively",
      "duration": 5599,
      "offset": "2082000"
    },
    {
      "text": "simple, but this workflow can work for more complex things, too. The key is to pay attention to what the models are",
      "duration": 6161,
      "offset": "2087599"
    },
    {
      "text": "struggling with. So here is my snitchbench benchmark and as I was showing here getting it to upgrade to",
      "duration": 6560,
      "offset": "2093760"
    },
    {
      "text": "the latest AI SDK was not trivial and something I've noticed as I have tried",
      "duration": 5279,
      "offset": "2100320"
    },
    {
      "text": "this many times first off is that there was a lot of things that were broken in cursors workree implementation that are",
      "duration": 5681,
      "offset": "2105599"
    },
    {
      "text": "now mostly fixed but the other thing I noticed was that the models would struggle a lot because they couldn't",
      "duration": 5760,
      "offset": "2111280"
    },
    {
      "text": "really run the project in a way to get good feedback. They would often try to run the whole project. And this is a",
      "duration": 7200,
      "offset": "2117040"
    },
    {
      "text": "benchmark that takes six to eight hours to run. So, it wouldn't actually know if the results were any good. So, I knew",
      "duration": 6960,
      "offset": "2124240"
    },
    {
      "text": "this would be a lot better if I could tighten the feedback loop. Models work way better when you give them the pieces",
      "duration": 5200,
      "offset": "2131200"
    },
    {
      "text": "they need to understand when things are and aren't working. If you can give the model the tools it needs to recognize",
      "duration": 6400,
      "offset": "2136400"
    },
    {
      "text": "something isn't working and then fix it, it's way fewer steps that you need to be involved in. You can let it go off, run",
      "duration": 6160,
      "offset": "2142800"
    },
    {
      "text": "again and again until it gets things working and then go take a look after. And I noticed a massive improvement in",
      "duration": 6000,
      "offset": "2148960"
    },
    {
      "text": "how reliably this set of tools could make changes to this project when I gave it some really basic tests. Not like",
      "duration": 7840,
      "offset": "2154960"
    },
    {
      "text": "unit tests, although those aren't a bad idea either. More implementation testing",
      "duration": 5120,
      "offset": "2162800"
    },
    {
      "text": "environment stuff. So here is an example of how I did all of this. Back when I",
      "duration": 5679,
      "offset": "2167920"
    },
    {
      "text": "was still on the V4 SDK, I wanted an easier test bed. So, I wrote this prompt. I want to make two significant",
      "duration": 6240,
      "offset": "2173599"
    },
    {
      "text": "changes to the project. First, I want tests to also define the set of prompts that they include. Instead of blindly",
      "duration": 6321,
      "offset": "2179839"
    },
    {
      "text": "reading all of the MD files from the test directory because previously this project had one folder named prompts and",
      "duration": 5439,
      "offset": "2186160"
    },
    {
      "text": "it would just read through all of them in alphabetical order, I want to split it up so I can have two versions. One",
      "duration": 5921,
      "offset": "2191599"
    },
    {
      "text": "that is doing tool calling with just basic math functions and one that's the real benchmark. But I also want to",
      "duration": 5440,
      "offset": "2197520"
    },
    {
      "text": "create a simple tool call test that we can use. I also said I want a dry run",
      "duration": 5440,
      "offset": "2202960"
    },
    {
      "text": "mode that will be run with this new simple tool call test because if I add",
      "duration": 5439,
      "offset": "2208400"
    },
    {
      "text": "these two things now I have the ability to tell the model, hey go run dry run,",
      "duration": 5280,
      "offset": "2213839"
    },
    {
      "text": "take a look at the outputs and see if it worked or not. And this was a massive improvement to the quality of the outputs. I also noticed that the results",
      "duration": 7121,
      "offset": "2219119"
    },
    {
      "text": "were often not type safe which was very annoying for me. So I added a one last piece as well. I think I merged this in.",
      "duration": 8000,
      "offset": "2226240"
    },
    {
      "text": "I did. So let's look at my recent commits. So first I did the dry run so that it would have the ability to run on",
      "duration": 5200,
      "offset": "2234240"
    },
    {
      "text": "cheaper models to get some feedback. But even the cheap models were taking forever. So I made additional changes that aren't in this branch. I'll just",
      "duration": 6639,
      "offset": "2239440"
    },
    {
      "text": "make a PR for it quick because I need to do this anyways. The updated runs. Create PR",
      "duration": 6321,
      "offset": "2246079"
    },
    {
      "text": "latest AI SDK fixes for reasoning stuff. basic math test for dry runs, type",
      "duration": 8160,
      "offset": "2252400"
    },
    {
      "text": "checking, etc. Now, I have this PR up, but the pieces I wanted to emphasize in",
      "duration": 5760,
      "offset": "2260560"
    },
    {
      "text": "here that have made things significantly better are uh package.json.",
      "duration": 7720,
      "offset": "2266320"
    },
    {
      "text": "Here I added the dry run command and implemented the simpler test that would get feedback faster. I also have tsc as",
      "duration": 7440,
      "offset": "2274320"
    },
    {
      "text": "a command here too. TSC is bunx TSC- noit. So this is a command to check the",
      "duration": 6160,
      "offset": "2281760"
    },
    {
      "text": "type safety of the project cuz I noticed a lot of models were generating code that wasn't type safe and I wanted to be",
      "duration": 5280,
      "offset": "2287920"
    },
    {
      "text": "able to tell the model, hey, make sure the results are type safe. I also updated the prompt that I use for this.",
      "duration": 5919,
      "offset": "2293200"
    },
    {
      "text": "I do actually have it saved because I use this as a test a lot. Obviously, you're not going to be saving a prompt that you're firing one time to revisit",
      "duration": 6081,
      "offset": "2299119"
    },
    {
      "text": "later on, but I use this as a model test, so I rewrote the prompt accordingly. I used to only use these",
      "duration": 6800,
      "offset": "2305200"
    },
    {
      "text": "first two pieces here. Upgrade the project to use the latest version of the ASDK version 5. I've attached a guide on",
      "duration": 6240,
      "offset": "2312000"
    },
    {
      "text": "how to do the upgrade below. Please make sure to review the guide before starting your implementation with this link to",
      "duration": 5599,
      "offset": "2318240"
    },
    {
      "text": "the official migration guide from AISDK from Verscell. Make sure that you use the latest version of the AISDK package",
      "duration": 6081,
      "offset": "2323839"
    },
    {
      "text": "as well as Zod and the open router provider. This is meant to be a hint cuz I noticed a lot of the time they weren't updating the other packages, so I just",
      "duration": 6000,
      "offset": "2329920"
    },
    {
      "text": "explicitly told it to. Test your implementation by running the following command. And this is where all of the new stuff was added. And these pieces",
      "duration": 6560,
      "offset": "2335920"
    },
    {
      "text": "here massively improved the likelihood that I get code out that works. I gave it the bun run dry run command. Each",
      "duration": 6240,
      "offset": "2342480"
    },
    {
      "text": "test should have a result written to the results dryrun directory. Make sure you validate these outputs and that the tool",
      "duration": 5040,
      "offset": "2348720"
    },
    {
      "text": "calls that they have to do include the parameters that were passed by the model. This is again I added this cuz I",
      "duration": 6240,
      "offset": "2353760"
    },
    {
      "text": "noticed that the generation wasn't putting this in properly like the markdown coming out didn't include the",
      "duration": 6079,
      "offset": "2360000"
    },
    {
      "text": "input. So I told it to fix that. I've had problems where the output ends up being empty objects instead of the expected output of what the model",
      "duration": 6161,
      "offset": "2366079"
    },
    {
      "text": "submitted in the tool call. I call out what went wrong in my prompt. Sometimes it's easier to go back and rewrite the",
      "duration": 5440,
      "offset": "2372240"
    },
    {
      "text": "original prompt with more context and more guardrails and a better harness than it is to tell the model to fix",
      "duration": 5280,
      "offset": "2377680"
    },
    {
      "text": "things. I often find like if the results just not working how you behave half or more the time it's better to restart",
      "duration": 6960,
      "offset": "2382960"
    },
    {
      "text": "update your prompt embed the learnings and if you can write tests or calls that",
      "duration": 5840,
      "offset": "2389920"
    },
    {
      "text": "it can do to verify it's not making the same mistake just like a junior engineer you need to give the AI models and the",
      "duration": 7040,
      "offset": "2395760"
    },
    {
      "text": "agents that you're working with the necessary tools and resources to work through the mistakes it might make so if",
      "duration": 6000,
      "offset": "2402800"
    },
    {
      "text": "you notice it making a mistake maybe go reset call out the mistake and maybe add",
      "duration": 5039,
      "offset": "2408800"
    },
    {
      "text": "some harnesses so that it can check these things before committing the changes. Because of the types being",
      "duration": 5121,
      "offset": "2413839"
    },
    {
      "text": "wrong, I added this piece here. Make sure all the TypeScript in the project is validated. We don't want to be shipping any unsafe code in this",
      "duration": 5520,
      "offset": "2418960"
    },
    {
      "text": "project. Please avoid using any and any other things that might break the type safety of the system. Make sure to",
      "duration": 5200,
      "offset": "2424480"
    },
    {
      "text": "validate the changes that you make using the bun runtsc command. This is an example of how I made this project",
      "duration": 5919,
      "offset": "2429680"
    },
    {
      "text": "behave way better by slightly changing the prompting. It is worth noting that more of the pieces here are me giving it",
      "duration": 7041,
      "offset": "2435599"
    },
    {
      "text": "harnesses than me telling it what I actually want. But that's fine. I honestly think a 50-50 ratio like this",
      "duration": 5760,
      "offset": "2442640"
    },
    {
      "text": "in a prompt for a complex enough project is fair. But the biggest learning here by far for me at least was how much",
      "duration": 6160,
      "offset": "2448400"
    },
    {
      "text": "smarter the models get when you give them these little nuggets. Once I created the dry run test, it was way",
      "duration": 6000,
      "offset": "2454560"
    },
    {
      "text": "cheaper to run. It was way more reliable and I got results that were what significantly closer to what I was",
      "duration": 6080,
      "offset": "2460560"
    },
    {
      "text": "expecting. I am curious how this review is going if uh Grapile is touching it. Is it? It did.",
      "duration": 9560,
      "offset": "2466640"
    },
    {
      "text": "Got a three out of five. Apparently, it's upset with something I",
      "duration": 5119,
      "offset": "2476240"
    },
    {
      "text": "did in the index.ts. Let's take a look at that. Well, another fun example here.",
      "duration": 6000,
      "offset": "2481359"
    },
    {
      "text": "This is when I start using the editor stuff like in the editor mode. I thought this would be a quick fix. I was going",
      "duration": 5361,
      "offset": "2487359"
    },
    {
      "text": "to go change it, but the timeout ID might not exist because of how let definitions work in Typescript. Very",
      "duration": 7040,
      "offset": "2492720"
    },
    {
      "text": "annoying. So, I know this code is fine, but we're getting a type error. So, I'm going to go to this line, press command K, and say, I'm getting a type error",
      "duration": 7440,
      "offset": "2499760"
    },
    {
      "text": "with this check. Please fix it. Let's see what it does.",
      "duration": 6840,
      "offset": "2507200"
    },
    {
      "text": "Still broken because it's not instantiated. It's not smart enough to do that. So, we will reject and pull in",
      "duration": 7520,
      "offset": "2514400"
    },
    {
      "text": "a smarter model. This is one of the times where I would still use the editor's like agent mode here. Command N",
      "duration": 6080,
      "offset": "2521920"
    },
    {
      "text": "for a new piece here. I don't trust composer for something like this. So, I'm just going to use opus because it's",
      "duration": 5040,
      "offset": "2528000"
    },
    {
      "text": "the closest one for me to click. Timeout ID is showing a type error in the",
      "duration": 6160,
      "offset": "2533040"
    },
    {
      "text": "finally block due to potentially not being instantiated. Fix it, please.",
      "duration": 8000,
      "offset": "2539200"
    },
    {
      "text": "Okay, let's see the change it made. It made change 306. It made the type of timeout ID return type or undefined. So",
      "duration": 5680,
      "offset": "2547200"
    },
    {
      "text": "now we're not getting that error case because this is this is a fun little typescript thing actually says we define",
      "duration": 5280,
      "offset": "2552880"
    },
    {
      "text": "let timeout ID with this specific type but there are conditions where hypothetically this might not be set. So",
      "duration": 6000,
      "offset": "2558160"
    },
    {
      "text": "we don't set it immediately. The type for timeout ID was no.js timeout but we",
      "duration": 5760,
      "offset": "2564160"
    },
    {
      "text": "don't know if it's instantiated or not. So the two types it could be are the right type or uninstantiated. By doing",
      "duration": 7439,
      "offset": "2569920"
    },
    {
      "text": "or undefined here, we are telling Typescript it's okay for us to touch this even if it doesn't exist because",
      "duration": 7681,
      "offset": "2577359"
    },
    {
      "text": "the non-existing case is now something we've encoded. Cool. I didn't really think that would be the solution, but I",
      "duration": 6079,
      "offset": "2585040"
    },
    {
      "text": "learned something. And that's again the cool stuff with these tools. I had other ways to solve this. I personally wouldn't have used let but to each their",
      "duration": 6960,
      "offset": "2591119"
    },
    {
      "text": "own. Now I learned something and made an improvement. I even bet I can delete those. And this is fine. Let's keep",
      "duration": 7361,
      "offset": "2598079"
    },
    {
      "text": "those changes. And now the non-null assertion is gone.",
      "duration": 6000,
      "offset": "2605440"
    },
    {
      "text": "Remove nonnull assertions. Get push. And now Grapile can do an",
      "duration": 6159,
      "offset": "2611440"
    },
    {
      "text": "updated review. And now we are golden. Seven files reviewed, no comments. Did",
      "duration": 6401,
      "offset": "2617599"
    },
    {
      "text": "it update their review earlier? I bet they did. PR safe emerge with minimal risk. index should be monitored for the",
      "duration": 6319,
      "offset": "2624000"
    },
    {
      "text": "custom tool loop behavior with various providers, especially the thinking block handling logic. That's because of a change anthropic made to how it persists",
      "duration": 5681,
      "offset": "2630319"
    },
    {
      "text": "thinking blocks that broke a ton of [ __ ] It works, but yeah, Michaela asking, I mentioned coming around on",
      "duration": 5680,
      "offset": "2636000"
    },
    {
      "text": "planning a little bit ago in tweets. How good is it really? It's significantly better because you can use a smart model",
      "duration": 5840,
      "offset": "2641680"
    },
    {
      "text": "to write a thorough plan and it's not going to get lost in the weeds of like going down weird tanges writing the",
      "duration": 5520,
      "offset": "2647520"
    },
    {
      "text": "code. it can write the plan and then you can have a dumb model, go implement it, notice anything that's wrong, and then",
      "duration": 5680,
      "offset": "2653040"
    },
    {
      "text": "go back and adjust the plan. Actually, that's a good tangent I should go on here. So, earlier I was showing you guys",
      "duration": 6160,
      "offset": "2658720"
    },
    {
      "text": "this improved prompt that I wrote to try and get better results when doing harder",
      "duration": 6800,
      "offset": "2664880"
    },
    {
      "text": "tasks. This is another part of why I think planning is actually really good and underrated for anything even",
      "duration": 6000,
      "offset": "2671680"
    },
    {
      "text": "somewhat vaguely complex. When you write a prompt like this and you tell it to go",
      "duration": 5200,
      "offset": "2677680"
    },
    {
      "text": "write a plan based on this prompt, then you take that plan and run a dumb model against it and it makes mistakes. You",
      "duration": 7360,
      "offset": "2682880"
    },
    {
      "text": "can look at those mistakes and then tell the planning model to go update the plan based on those mistakes and then rerun.",
      "duration": 6640,
      "offset": "2690240"
    },
    {
      "text": "One of the important things to like get into your head when you're building this way is that the code is cheaper now.",
      "duration": 6560,
      "offset": "2696880"
    },
    {
      "text": "Like way cheaper. You shouldn't get fixated on a specific branch you're going down with the agents because you",
      "duration": 7919,
      "offset": "2703440"
    },
    {
      "text": "can just throw that away. It's silly, but when you're doing these types of like higher level architectural changes",
      "duration": 7281,
      "offset": "2711359"
    },
    {
      "text": "using agents, treat the prompt as the thing that you're maintaining between attempts, not the code. Because if you",
      "duration": 7120,
      "offset": "2718640"
    },
    {
      "text": "build this plan and run it against a dumb model and it makes two dumb mistakes, the code it just generated was really cheap and it did it really fast.",
      "duration": 6559,
      "offset": "2725760"
    },
    {
      "text": "You don't need to try and force it to fix that code when you can just throw it away. Tell the planning model to change",
      "duration": 5921,
      "offset": "2732319"
    },
    {
      "text": "the plan very slightly and then rerun it with the dumb model and regenerate the whole thing. It's a lot easier to keep",
      "duration": 6240,
      "offset": "2738240"
    },
    {
      "text": "it from going down these weird paths if you just throw out the weird path and try again. I would say at this point",
      "duration": 6400,
      "offset": "2744480"
    },
    {
      "text": "when I'm doing these heavier tasks, for every like one big change I hit accept",
      "duration": 6160,
      "offset": "2750880"
    },
    {
      "text": "in merge, there were like four or five attempts that made me go back to the original prompt or the original plan to",
      "duration": 6720,
      "offset": "2757040"
    },
    {
      "text": "adjust. That said, there are some higher level things you can do to make it more likely to be successful on that first",
      "duration": 6079,
      "offset": "2763760"
    },
    {
      "text": "attempt. One that I haven't tried just yet personally, but I've heard really good things about from Ben is cloning",
      "duration": 5841,
      "offset": "2769839"
    },
    {
      "text": "the repos for the projects that you're building around. So, if you're using",
      "duration": 5600,
      "offset": "2775680"
    },
    {
      "text": "effect heavily, effects is a it's a language. It's called a package, but it",
      "duration": 5600,
      "offset": "2781280"
    },
    {
      "text": "really is a different language built into Typescript, it takes a bit to get used to, and their docs kind of suck.",
      "duration": 5760,
      "offset": "2786880"
    },
    {
      "text": "The onboarding is awful, and there's a lot of functionality that's just not included in here. So, if you want to",
      "duration": 5679,
      "offset": "2792640"
    },
    {
      "text": "take more advantage of effect and make sure you're using the right pieces, telling it to go index the docs isn't",
      "duration": 6161,
      "offset": "2798319"
    },
    {
      "text": "going to get you very far. I've had awful luck with tools like context 7 is just not good enough. Supposedly, you",
      "duration": 7520,
      "offset": "2804480"
    },
    {
      "text": "can clone the effect repo as a subree, give it to Claude, and tell it to use that repo as docs. In particular, the",
      "duration": 7040,
      "offset": "2812000"
    },
    {
      "text": "tests in that repo, the tests directory or the test files in a given project end",
      "duration": 5200,
      "offset": "2819040"
    },
    {
      "text": "up being really useful to the models for these things. because it's full of good examples. Models and agents do",
      "duration": 6320,
      "offset": "2824240"
    },
    {
      "text": "significantly better given a handful of specific things. The first, which we just talked about, is a plan. A prompt",
      "duration": 6480,
      "offset": "2830560"
    },
    {
      "text": "is not good enough. A prompt can be used to write a plan. And once you verify the plan, the plan could be used for the implementation. Another really useful",
      "duration": 6799,
      "offset": "2837040"
    },
    {
      "text": "thing is a verification harness, some method it can use to know things",
      "duration": 6161,
      "offset": "2843839"
    },
    {
      "text": "work. This could be tests it can run a simpler version of the app it can run type checking and specific commands and",
      "duration": 6800,
      "offset": "2850000"
    },
    {
      "text": "scripts you give it in something like JavaScript you can in the package JSON write custom scripts it can run those",
      "duration": 5279,
      "offset": "2856800"
    },
    {
      "text": "types of things that put output and standard out that it can read and see and learn from not learn like get",
      "duration": 6161,
      "offset": "2862079"
    },
    {
      "text": "smarter but learn like oh I made this mistake I should go fix it really helping reduce that cycle and that loop",
      "duration": 6400,
      "offset": "2868240"
    },
    {
      "text": "of trying things and fixing them one of the biggest by far context and I don't",
      "duration": 5600,
      "offset": "2874640"
    },
    {
      "text": "just mean context on all the things in your project. I mean context in the things it has access to availability to",
      "duration": 7200,
      "offset": "2880240"
    },
    {
      "text": "and examples for how it should do those things. I've actually found that a lot of agents get better in big projects",
      "duration": 6720,
      "offset": "2887440"
    },
    {
      "text": "because once they're indexed, it has lots of examples of good patterns and also probably some examples of bad",
      "duration": 5679,
      "offset": "2894160"
    },
    {
      "text": "patterns that it can use in reference when it is developing and iterating. Having those examples is really really",
      "duration": 5841,
      "offset": "2899839"
    },
    {
      "text": "helpful. And if you write a lot of projects using the same few packages, it's not the worst idea to yoink some of",
      "duration": 6480,
      "offset": "2905680"
    },
    {
      "text": "that code over to point at and be like, \"Hey, here's how I like to do things.\" If you can paste one or two examples in",
      "duration": 6240,
      "offset": "2912160"
    },
    {
      "text": "your prompt, good chance it'll increase the quality of your output. But once the codebase has enough things or god forbid",
      "duration": 6400,
      "offset": "2918400"
    },
    {
      "text": "you write some tests to have this context in your codebase, quality of the outputs you're going to get, it's going to go up a ton. If you want to try the",
      "duration": 6640,
      "offset": "2924800"
    },
    {
      "text": "embedding the repost thing out, Ben, my channel manager, who also has a YouTube channel, you should check out his",
      "duration": 5280,
      "offset": "2931440"
    },
    {
      "text": "channel by the way, Faze, show it on the screen and I'm sure we'll have a link in the description. He's talking a lot about this stuff and he's coding way",
      "duration": 6320,
      "offset": "2936720"
    },
    {
      "text": "more than I am right now. He threw up a repo for this for those who want to try it out themselves. Works with cursor and",
      "duration": 5279,
      "offset": "2943040"
    },
    {
      "text": "open code. I'm personally finding myself in cursor much more than any of these CLI tools. The CLI tools just don't have",
      "duration": 6161,
      "offset": "2948319"
    },
    {
      "text": "a good enough workflow for when you're changing things or want to edit things or do things. but also they don't have",
      "duration": 6720,
      "offset": "2954480"
    },
    {
      "text": "stuff like LSP support, so they don't know when the types are wrong unless you give them a command they can run, which",
      "duration": 5040,
      "offset": "2961200"
    },
    {
      "text": "is super annoying. I really like having the integration between my editor and my agent flow. And I really like the agents",
      "duration": 6960,
      "offset": "2966240"
    },
    {
      "text": "view when I'm doing this. I almost treat the editor view like a like a review",
      "duration": 5119,
      "offset": "2973200"
    },
    {
      "text": "view where I'm in here to look at the code and if the models are just not doing things right, I'll sit here and",
      "duration": 6161,
      "offset": "2978319"
    },
    {
      "text": "code it out. Sometimes I need to just go write the index ts or write out exactly how I want to call things. I'll often",
      "duration": 7280,
      "offset": "2984480"
    },
    {
      "text": "pretend that the API I want exists and write out the code and leave all the type errors in for how I want to work on",
      "duration": 7359,
      "offset": "2991760"
    },
    {
      "text": "a thing or how I want it to behave. I'll just write the exact code I want knowing nothing works yet and then tell the",
      "duration": 6161,
      "offset": "2999119"
    },
    {
      "text": "models or tell the agent make me a plan that will fulfill this functionality so",
      "duration": 5120,
      "offset": "3005280"
    },
    {
      "text": "this behaves. And then I go through my usual loop. And again, my loop is make a prompt, create a plan, edit the plan,",
      "duration": 7679,
      "offset": "3010400"
    },
    {
      "text": "run the plan, and if the plan doesn't work, closely inspect the run, see what",
      "duration": 5201,
      "offset": "3018079"
    },
    {
      "text": "failed, what it got wrong, why it got it wrong, go back, adjust the plan",
      "duration": 5200,
      "offset": "3023280"
    },
    {
      "text": "accordingly, and rerun. And I go through this again and again. Or, and this is",
      "duration": 5040,
      "offset": "3028480"
    },
    {
      "text": "where you really need to like get a good gut feel and taste. If it seems like it's just so far off from what you're",
      "duration": 5280,
      "offset": "3033520"
    },
    {
      "text": "trying to get it to do, you need more information, more context, more harness",
      "duration": 5200,
      "offset": "3038800"
    },
    {
      "text": "for it. At that point, go write some tests that can execute, write an example file that shows what you're trying to",
      "duration": 5839,
      "offset": "3044000"
    },
    {
      "text": "do, then maybe try the agent again or just go finish writing the code. It's a lot easier sometimes to just go do it.",
      "duration": 7041,
      "offset": "3049839"
    },
    {
      "text": "And I still find myself doing that here and there. A lot of it's just gut feel. Like you got to recognize like this is",
      "duration": 5280,
      "offset": "3056880"
    },
    {
      "text": "going to take too much work to make the model behave. it's better if I just go do it myself. But sometimes it's worth",
      "duration": 5600,
      "offset": "3062160"
    },
    {
      "text": "pushing through that because you might be surprised. You might learn more of these prompting skills and general like",
      "duration": 5520,
      "offset": "3067760"
    },
    {
      "text": "agentic flow things and get better. Or you might just have a really funny thing to tweet about how stupid AI models are.",
      "duration": 6960,
      "offset": "3073280"
    },
    {
      "text": "Either way, it's pretty beneficial to go down these rabbit holes at times. But don't be scared to move off it. And I",
      "duration": 5599,
      "offset": "3080240"
    },
    {
      "text": "know I've emphasized this a lot in other videos, but I don't know if I have here yet, so I'm going to triple down on this point. Do not use these models to build",
      "duration": 7280,
      "offset": "3085839"
    },
    {
      "text": "things you don't understand. These agents are great for doing a lot of work that you already know how to do but",
      "duration": 6561,
      "offset": "3093119"
    },
    {
      "text": "don't feel like doing. I find myself spinning up one-off projects and sandboxes for experiments way more. I",
      "duration": 5600,
      "offset": "3099680"
    },
    {
      "text": "find myself writing tests and example cases way more. I find myself writing formalized plans way more. I'm doing",
      "duration": 6160,
      "offset": "3105280"
    },
    {
      "text": "these things because it's way faster and easier than it's ever been. And the result is better code than I would have",
      "duration": 6240,
      "offset": "3111440"
    },
    {
      "text": "written by hand or code being written when I would have just moved on from this project. Like I'm gonna go play a",
      "duration": 6000,
      "offset": "3117680"
    },
    {
      "text": "ton with this autodraftify thing when I'm done streaming. This is an idea I had last night when I was falling asleep. I wrote it down on my phone and",
      "duration": 5919,
      "offset": "3123680"
    },
    {
      "text": "then I just went and coded it in what, like 10 minutes. That's so cool. I've always been a fast developer, but",
      "duration": 5841,
      "offset": "3129599"
    },
    {
      "text": "previously this would have taken me like 30 minutes to an hour and I would have had to be sitting there focusing the whole time. Now I can do it in five to",
      "duration": 6960,
      "offset": "3135440"
    },
    {
      "text": "10 while working on other stuff in the background and playing with all these fun tools. That's awesome. I feel like",
      "duration": 5760,
      "offset": "3142400"
    },
    {
      "text": "I'm developing a whole new set of skills and there's a good chance that my take on all of these things is going to change a ton even by the time the video",
      "duration": 6399,
      "offset": "3148160"
    },
    {
      "text": "is out. But this is a rough idea of the workflow I have found myself using when I play with these things and when I",
      "duration": 6401,
      "offset": "3154559"
    },
    {
      "text": "build stuff with it. And I found it to just be awesome. Also, believe it or not, I know Curse is a sponsor. Call me",
      "duration": 6720,
      "offset": "3160960"
    },
    {
      "text": "a shell. Call me whatever. I am still on the $20 a month tier. I have never had to pay an overage fee and I'm doing this",
      "duration": 6560,
      "offset": "3167680"
    },
    {
      "text": "[ __ ] a ton. It's not super expensive to do all these things. I know there's a stereotype of like the AI agents are so",
      "duration": 6720,
      "offset": "3174240"
    },
    {
      "text": "expensive, why not just go do it yourself? If you use the expensive ones for something relatively cheap and low",
      "duration": 5840,
      "offset": "3180960"
    },
    {
      "text": "on token utilization like planning, then you use a cheap fast model like a grot code, composer, whatever else to do the",
      "duration": 6400,
      "offset": "3186800"
    },
    {
      "text": "implementation. It ends up not costing that much at all. I'm still very much in",
      "duration": 5200,
      "offset": "3193200"
    },
    {
      "text": "the green on my cursor usage for my current subscription. And I know the majority of people I work with have not",
      "duration": 5520,
      "offset": "3198400"
    },
    {
      "text": "even gotten close to hitting those limits. So yeah, 20 bucks a month to",
      "duration": 5600,
      "offset": "3203920"
    },
    {
      "text": "fundamentally change how I wrote code. For me, that's really worth it. As a person who was super skeptical of cursor and was scared to invest initially",
      "duration": 6240,
      "offset": "3209520"
    },
    {
      "text": "because didn't think it was going to be for me. Now I love this team. I love this editor. It's how I prefer to work.",
      "duration": 5200,
      "offset": "3215760"
    },
    {
      "text": "And while I do enjoy experimenting with new tools and never shy away from trying something if it's interesting to you,",
      "duration": 5359,
      "offset": "3220960"
    },
    {
      "text": "like if you're curious how Gemini CLI works, go give it a shot. If you're curious why everybody's so excited about",
      "duration": 5121,
      "offset": "3226319"
    },
    {
      "text": "Cloud Code, go try it. If you're curious about what's going on with Codeex and why they rewrote in Rust, I can't give you a good answer, but I can tell you to",
      "duration": 6159,
      "offset": "3231440"
    },
    {
      "text": "go try it. But if you're not interested in those things, don't feel like you have to. It's totally okay to get to the",
      "duration": 5281,
      "offset": "3237599"
    },
    {
      "text": "stuff a bit late. I think we're now at the point where most developers should start playing with agents in figuring",
      "duration": 5760,
      "offset": "3242880"
    },
    {
      "text": "out what their agentic workflow looks like. But you shouldn't feel like you have to go learn all of these tools,",
      "duration": 5760,
      "offset": "3248640"
    },
    {
      "text": "you're going to get fired. You should take this opportunity to try these things out. See what they are good at.",
      "duration": 5439,
      "offset": "3254400"
    },
    {
      "text": "See what they're bad at. And one last spicy take, getting better at this stuff will make you a better coworker. Because",
      "duration": 7280,
      "offset": "3259839"
    },
    {
      "text": "getting better at clearly giving instructions to your tools and getting better at giving clear reproduction",
      "duration": 6960,
      "offset": "3267119"
    },
    {
      "text": "steps, things like tests and harnesses to the agents to write this code. Once",
      "duration": 5921,
      "offset": "3274079"
    },
    {
      "text": "you get good at doing that, you've improved your communication skills massively. And now when you work with",
      "duration": 6079,
      "offset": "3280000"
    },
    {
      "text": "co-workers or if you become a manager and have your own employees on your own team, your ability to communicate with",
      "duration": 5760,
      "offset": "3286079"
    },
    {
      "text": "them will go up too. This is a stress test on how clear and concise you can be as you build. This is also why I hate",
      "duration": 6881,
      "offset": "3291839"
    },
    {
      "text": "all those MCPs that just fill up your context window with a bunch of garbage documentation. No, [ __ ] that. Get better",
      "duration": 5839,
      "offset": "3298720"
    },
    {
      "text": "at writing. Get better at clarifying what you want and get better at writing the necessary tools that allow for the",
      "duration": 5520,
      "offset": "3304559"
    },
    {
      "text": "agents to know if they succeeded or failed at their goals. I have been loving this. I've genuinely had so much",
      "duration": 5681,
      "offset": "3310079"
    },
    {
      "text": "fun building in this way. I feel rejuvenated writing code in a way I haven't in a long time. And while there",
      "duration": 5440,
      "offset": "3315760"
    },
    {
      "text": "are still lots of tasks that I find the agents just aren't that great at, especially like big overhauls using",
      "duration": 5600,
      "offset": "3321200"
    },
    {
      "text": "underdocumented technologies, man, it's so fun to play with this stuff. Highly recommend giving it a shot if you",
      "duration": 5440,
      "offset": "3326800"
    },
    {
      "text": "haven't just yet. Let me know what you guys think. Am I a crazy AI pill vibe coding lunatic or is this actually",
      "duration": 5920,
      "offset": "3332240"
    },
    {
      "text": "useful? Let me know in your own workflows and tell me how you've been coding with AI in the comments. And until next time, keep coding.",
      "duration": 7600,
      "offset": "3338160"
    }
  ],
  "transcriptText": "Remember that video I did about how I was blown away with Copilot all the way back in like 2023? It's been almost 2 years since that video was published, and the way I use AI to write code has changed more than I ever would have imagined. I'm still on Cursor, but the ways I use it have changed so fundamentally that I wanted to try and break down what my workflows look like when I code with AI. I have been building a lot more recently, especially these last few days. And I think I've really settled into a flow that I like enough to share with you guys. From how I use AI models for planning, execution, how I do reviews, how I use work trees, how I've kind of changed how I use Git on a fundamental level. I'm really happy with where I've landed and how I integrate AI into my workflows for my day-to-day coding tasks. It has made me much more productive and overall having a lot more fun doing my engineering work as a result. I really want to emphasize a few things before we dive in though. Thing one, this is not a Vibe coding tutorial. We're not here to recommend ways to write code without understanding code. If you don't know how to code, go learn how to code. Come back here after. This video is not for people who don't know how to write code yet. This is how to use these tools as an existing productive software engineer to be more productive. The other thing that's important to note is that we'll be talking about a lot of companies that I've invested money in because I like the space and when someone does something I enjoy, I try to throw money at them because it's a good chance if I like it, they're going to find success. So, account for all sorts of biases throughout this video. We're here to talk about the engineering work though, not about how I'm a paid shill for some thing that has made my life way better and more productive. I really want to showcase how I use these things every day and the tricks and tips I've learned from all the others around me. from how I work in my editor to how I handle code reviews to how I work on stuff with AI. None of the tools we're talking about here paid me to make this video. In fact, I wasn't paid at all for this. Okay, I'm about to be paid by today's sponsor. Quick question. If I was to spam your most expensive endpoint with thousands of requests, how much would it cost you? If this was like 5 years ago, it would cost you almost nothing. But nowadays, with all these endpoints that are backed by LLMs, a given request, just one request can cost you multiple dollars. If I spam you with hundreds or thousands of them, how are you going to handle that? How much is it going to cost you? How easily can a malicious actor screw your entire service by just hitting the right endpoint the right way? How reliable is the code and the firewall and everything else you put together? Can you see it in code review? Does it work in the beta builds? Does it work in your test environments? How do you actually know? I know because with recent projects I've been building with today's sponsor, ArcJet. These guys make it so much easier to secure your services because they're not some crappy dashboard you have to go click all the right buttons in and pray you did it right. They're just code. Here's how insanely complex it is to set up. You install their package. You add your API key and then you copy paste this code block that is mostly comments that are just commenting out things that you might want to use. So how do you actually use it? You go to your get request. You have con decision equals await aj.protect. pass it the request and it tells you the decision and even tells you the reason. But what happens if the user can access the thing but they shouldn't access it too much? You might build your own crazy complex rate limiting service that's split across 15 things like we had to for T3 chat. And if I was building T3 chat today, that's not how I would do it. I would use the token bucket from ArcJet because it's so much simpler. Remember that rules array before? Now you can also define a token bucket in it. Mode characteristics. This is how you're identifying the characteristic for this user and like separating them from others so that three different users aren't hitting the same bucket. A refill rate, an interval, and a capacity. Couldn't be simpler. You have the user ID and you pass that to AJ.protect. And you could even specify how many of a thing is requested because you might be managing how many tokens a user can spend and different models cost different amounts. It's these things are annoying to do. They make it way easier. It's just code. No dashboards, no weird config files, no YAML, no [ __ ] You just write TypeScript code in your TypeScript project and now you're safer. If your service isn't secure, I don't blame you. Getting it right was way too hard to do before. It no longer is thanks to ArcJet. Check them out now at soyv.link/archjet. On one hand, I'm tempted to go into an existing project and we probably will in a bit, but I want to start with something new to show you guys from like the start how I use AI to scaffold, plan, and build my projects. Obviously, AI stuff works really well for green field and small projects. You can make it work great for big ones as well, especially if you use planning properly. But for now, we're going to start a new project to really showcase the strength of these tools. And hopefully cursor won't break because I've had a rough time with curs last few days, working closely with them to get all these bugs fixed. But we'll get there. So, I have an idea for a project that I want to build here with you guys. I am interested in the quality of writing across various models and comparing the writing styles. I also had this random idea of wouldn't it be cool if you had a model that's better at writing give feedback to another model about the thing it wrote. So what I want to create is a tool that has one model write a draft for a prompt. Another model give feedback and then the first model applies the feedback and you can compare these two resulting drafts. I think this is going to be fun. First thing I recommend is that you actually init your projects yourself. I know the AI can run commands and do all these things, but it's a lot easier to get it right if you control the initial environment. Using stuff like create T3 app, better tstack, or even just a create next app or bunit will help your tools start in a much better place. So that's what we're going to be doing here. Bunanet blank because I don't want this to be a web app. And now we have all the files in here. Going to open cursor up with this project. Right now we're in the editor view. We'll go back to the editor view in a bit. I'm going to use the command E hotkey to switch over to agents mode in cursor. I have found myself genuinely really liking the agent mode. It's a really nice way to plan things out and look at the code as a result of the work you're doing rather than squeezing this chat interface into a traditional text editor. I find myself switching between the two a lot. My only wish and uh we'll be talking a lot about this in the future cursor. I'm not giving them UX feedback yet because they haven't earned the right to it. There's too many bugs they have to fix first. But one of the pieces of UX I would like for them to change is that the editor view has the agent hidden by default. And when I switch to and from it, the agent only becomes visible in editor when I hit the key. The same way the editor isn't visible in agent by default. We have a new project. Nothing happening here. Opus is my model. Opus has been the model I'm using for most of my code recently. But I don't want to code just yet. I want to plan. So let's plan this project out. I know a couple of the tools that I want to use for this. I could ask the model to set it up and I will here for the sake of demoing, but I know I want to use the AISDK and that I want to use open router. So, traditionally I would probably still go set those up manually because it's not that hard to do, but I'm not going to do that just yet. One thing I am going to do though is set up some pieces for my workspace and cursor. It seems like bun and nit is now smart enough to specify in cursor rules in this directory here to use bun instead of node vit npm and pnpm. That's a good thing that they're adding. I am pumped that they are now ahead of the curve in telling your editor to not use things it shouldn't be. But we're going to be using work trees. So I'm just going to copy this setup script I have here because this setup script does two things. It uninstalls whenever a new work tree is created. And it also copies thev file from the root to this new work tree. We'll talk about what work trees are and why I like them so much in a bit. We're not there just yet. But I'm going to just paste this file in for us to get started with. I'll make sure this repo is available publicly if you want to grab any of this code. Normally the work tree configurator comes up as part of your setup when you run a work tree for the first time, but I just like having it by default. I might even make a new template for like how I start projects with all my cursor stuff in the future. I don't use that much though. I'm relatively minimal on rules and tools and all the other things that I add. I don't use any MCP servers or any stuff like that. Don't find it necessary. So the order of events we're about to engage in, we're going to start with planning to make a plan for how we want to build and then we're going to do the building. This needs to be a git repo. So I'm going to init quick. Now we have our initialized git repo. We want to give enough context for what we want it to plan before we start executing the plan. I could type out all that context, but honestly I find that I give more context when I use my voice than when I type. I really really like using something like Whisper Flow, which is going to hopefully be a sponsor soon if all goes well. When I'm trying to do this type of prompting, I could type it all out, but I'm not going to type as much as I'll talk. And sometimes I'll realize things as I'm talking, which is nice. Let's tell it what I want. This project is going to be a demonstration of how different AI models handle writing tasks. My goal is simple. Generate an essay based on a prompt the user provides. have a different AI model review this essay and give feedback and then hand that feedback back to the original model to update the essay accordingly. All three of these steps should be saved as markdown files when the generation occurs so that I can easily review them in a text editor. Use the AISDK by versel and open router in the open router provider for the AISDK for the provisioning of the inference. Make sure everything you write is type safe. I think that's enough. This is a decent enough starting point. I could do a bunch of bullet points and things, but I think this is fine. Opus seems to be a good enough planning model, but honestly, I still have a soft spot for GPT 5.1 for planning. It's not the best at tool calling reliably. I've honestly found that the models that call tools the best are most of the anthropic ones as well as composer 1. We'll show you how I use all of those momentarily because first we have to submit our planning. Now it's going to ask us some questions. How do we want to use this essay review revision pipeline? Simple CLI script I run manually that asks for a prompt then writes three markdown files. That sounds right to me. CLI tool that takes the prompt as command line args or standard and writes the files. That could work. Definitely don't want HTTP and I definitely don't want a web UI. So we'll do that for the two roles. SA writer versus reviewer. How do you want to choose models? Uh hardcode specific route models for now. I like that plan. Continue. And now it is creating a plan. Now this part is very important and might be hard for you vibe coding folks. See this plan? I know this sounds crazy, like absurd, but hear me out. You should read the plan. I know just like what you should do work when you're using AI. Isn't that the whole point? You could ask another AI to do it for you, but you should read the plan. Sorry, the the plan.PlMD plan.plan. That's how you know it's going to be good. So let's take a look here. Implement a bun friendly TypeScript CLI that prompts the user for an essay topic. Uses model A overview SDK to generate essay, model B to review the essay, produce feedback, and then model A again with the feedback to produce a revised essay. Saves all three artifacts as markdown files on disk in a consistent location. High level design details. Keep using bun with index main CLI AI client setup. Add AI open AI in the open router provider. Well, let's give it some feedback. Why are you including the OpenAI package? We're going to use Open router, not OpenAI. Now, it will adjust that. You can keep reading. While we do that, configure a small AI client module or inline an index ts a very small that wires the SDK to open router using an open router API key environment variable. Hardcode the two model IDs, one for generation, one for review with clear cons name so you can easily change them later. Now, the pipeline orchestration. Implement a run essay pipeline function. Reads the prompt from standard in or default if standard in is empty. Calls the essay model with a system prompt plus a user prompt to generate the initial essay. Call the review model with system instructions plus the essay contents to generate structured feedback. Call the essay model again. Keep everything strongly typed. The markdown file output. Okay. Simple naming scheme. Runs timestamp essay. Time stamp review time stamp revision. That looks fine. bun node FS APIs include basic front matter or headings. All looks good to me. Implementation steps. It left the OpenAI package here. More changes. Just going to write these all out once cuz a good chance we're going to need more. You still have OpenAI in the setup depths section. AI client instantiate the SDK with the open order provider. Exposes typed helpers like generate essay, review essay, and revise essay. pipeline logic implement the run say pipeline interactively ask for prompts via standard in duh this all looks good to me I'm going to just grab an env file quick because I already have one that has everything I need for this make sure the runs directory is included in the git ignore this all looks good so far I could just hit the build button here or here but I'm going to do things a a little differently cuz I have separate preferences. Here's where we're going to start using work trees. If you're not familiar with work trees, it's a way to take a git branch for a project and put it temporarily in a different directory. So, you can have multiple branches on your machine at the same time. This is really useful for when you're doing different work in parallel. It's incredibly useful if you're working on a task. You notice something else you want to touch, but you don't want to clog up the PR you're currently working on. So you can spin up another agent with a work tree that will be given a new directory just for it to work in. And once you're done, you can choose to merge in those changes. It's somewhere between a background agent and traditional like the agent controlling your editor. There's like the three layers of how deep these agents go, like I guess how far they are from the work you're doing. Traditionally when you like command I in your editor and have the little sidebar that is using the agent to control the editor and change code in your directory direct like right there. There's backard agents where it clones your repo in the cloud and does edits in the cloud that you can then merge in or make a PR for when you want and then work trees which are somewhere between the two where they run on your machine but in parallel to your other stuff. They're also really useful because you can use it to compare different models. So here I have composer, opus, sonnet, and gemini all selected. And if I wanted, I could change the number of each of them being used. I could have three instances of composer running over this task at the same time. I'm going to use just one for composer, opus, and screw it, Gemini 3 pro. People like that model, right? Cool. These are the models I'll use. Obviously, Composer 1 is the really fast, relatively cheap one that was built by our friends at Cursor. I do like it a lot for these types of things. Opus will be fun to see how it behaves here, and Gemini 3 Pro will be as well. So if it turns out the DOM model is not good enough for this, I could use one of those instead. And here I can reference the plan by just adding it. Now it's here. I have the set for work trees based off of main. So this is the branch it's going to use when it creates the work trees. And I'll tell it implement the project. Send. And now we have three different instances running in their own paths in their own branches doing this implementation. And if we want, I can go to any one of these in my terminal. I bullied them into adding this little button here, copy workree path. Previously, they only had open terminal in workree, which would open the terminal in the VS Code, well, cursor instance, which I hated. But now, copy workree path. Go into your terminal, make a new tab, cd, and here is the path. Notice it's inturs. That's where they dump all of these work trees by default. And it has all the files for this branch. This is also why I introduced the work trees JSON file because when a new work tree is created, I want it to run bun install and also copy over the environment variables. So if we go back here, you'll see if we do ls- aenv is here, which it wouldn't have been otherwise because it wouldn't copy the git ignore files otherwise. Let's see how the agent is doing. Oh no, can they not find the cursor? I I'm going to send a very rudely worded message to my shared Slack with cursor. One moment, please. Rude DM sent. Let's go back here. Let me choose how I want to deal with this. There are options. One of the easiest by far is to just copy the plan file and put it in your editor. Ideally, by the time you're doing this, they will have this [ __ ] fixed. But right now, it is entirely [ __ ] broken. So, I'm doing my best. I'll put this in dot cursor. Let's try again. Work tree, same models at planmd implement the plan and build out the project. And this time it can access the plan because I put the file in the repo and it will get copied over and it created the to-dos. Now we have all three of these models going and we can see just how much composer flies. I have to allow it to run commands. Except Except. Yeah, Composer flies. It's not the smartest, but it's pretty dang good. God, it goes fast. It's almost done, and the rest are like one to three steps in, and none of these are even Gemini, like the slow model. Obviously, you don't need to run this with three different models at the same time. It's a flow I like because I like to compare the results and learn more about the models whenever I can. A lot of this should be taken as a learning opportunity and yeah, I like learning. So, let's go hop into this directory and see how that goes with the composer implementation. You can just read it right here if you want using the review tab, which I've actually grown to really, really like. The review tab is awesome because it groups all the files that were changed into one single view that you can scroll through, which is super handy. So, let's just read through it first and see. Has a run directory interface for the pipeline output. Uh async ensure runs directory that checks if the directory exists. Get timestamp string now is new date. Cool. Write pipeline outputs. We have the index. So prompt for topic. Enter your essay topic. You write the topic essay results await generate essay topic. So this is when it goes to generate the essay. Then we have the review revision results. Revise essay. Write pipeline. Cool. And we have in here all of those functions. Results a way generate text review model. You're an expert writing tutor and editor. Review the essay provided and give constructive specific feedback on areas such as structure, clarity, argumentation, style, and areas for improvement. Be thorough but encouraging. Please review the following essay and provide detailed feedback. And then revise essay as the differences in prompt. You're an expert essay writer. Revise the provided essay based on the feedback given while maintaining the core message topic and improving the areas identified. Cool. This looks like a good starting point for me. Let's give it a shot. Copy work tree cd to the right place. Fun run index.ts essay topic. Let's come up with a good one. Write a thorough breakdown of why explicit return types in Typescript might not be a good idea even though they are in other languages. Now it's generating the initial essay. I forgot to specify models. What models did it pick? I'm scared. Used for mini and 3.5 sonnet. That's great. We can go through and change those momentarily. Let's go grab the slugs that I want. Open router.ai. I want Opus 4.5 the essay model and I want Kimmy K2 thinking for this one. Let's take a look at what it did in the interim. I'll often open up another cursor instance inside of the thing that I just created for the work tree because it's nice to be able to look at. So here's the essay case against explicit return types in Typescript. In the blossoming realm of web development, TypeScript has secured its position as a powerful supererset of JS. Yeah, this is so 40 it hurts. So let's try that again. Copy the same prompt. And if you guys were curious how fast the cursor team moves, I just got a message from them. They'll have a new build ready for me with the fixes I requested in 45 minutes. Kind of nuts. That team works hard. Essay was generated. Now it's reviewing. Revising essay based on feedback. Pipeline completed. Let's take a look. We have our new timestamp for the new run we just did. This one was written with Opus 4.5. The case is explicit return to TypeScript. When type inference shines already so much better than the 40 version. TypeScript has transformed JavaScript development by introducing static typings to a dynamically typed language. Among the many decisions typeser developers face is whether to explicitly annotate return types on functions or to allow the compiler to infer them. While languages like Java, C#, and Rust benefit greatly from explicit return type declarations, TypeScript occupies a unique position where mandatory explicit return types can actually work against the developer. This explores why TypeScript's type inference for the return types is often preferable to explicit annotations despite conventional wisdom from other type languages. Little verbose, not bad, though. Let's see what the review looks like. I'm curious. This is an excellent, well-argued essay on nuance types or topic. You struck a good balance between technical depth and readability. Your willingness to address counter arguments strengthens your position consistently. Let me offer detailed feedback to help elevate it further. The writing is clean and authoritative, though there are opportunities to sharpen certain arguments and vary your pros rhythm. Some section transitions feel abrupt. For example, between sections two and three, you could add a bridge sentence like, \"However, this power comes with a hidden cost when developers choose to override it.\" So that's between sections two and three. Yeah, it's they can also improve ID responsiveness when working with complex generic functions and then immediately to a different unrelated topic. Yeah, that is pretty sudden. I see why they would call that out. Let's see how the revised version came out. already like it more. Starting with an actual like pointed story, the first thing many devs do when learning TypeScript is annotate every function's return type. Second thing they do is spend hours maintaining those annotations after refactoring. This pattern reveals a fundamental tension in Typescript development. While languages like Java, C#, and Rust benefit greatly from explicit return type declarations, TypeScript occupies a unique position where mandatory explicit return types can actually work against the developer. Already so much better. Let's see how the transitions are now. Thanks. So the return type has here creates redundancy and maintenance overhead without providing additional safety. First off, I like this project. This is actually fun and useful to look into. And this is one of my favorite things to do with AI is when I have a theory, go code out that theory and experiment with it a bit. And now we have an experiment that's actually kind of cool and useful. I would love to do some type of like arena project where these can compete similar to like LM arena, but my initial thought is it'd be cool to have AI look at the two essays and say which one it thinks is better. I hope I've established that a dumb fast model is good enough for this once you have a good plan because the composer run was a lot faster and probably also a lot cheaper than both the Opus and Gemini 3 runs. We can take a look at the Gemini code, see if it's any better here. I like the essay system prompt better. So, I think I'm just going to merge this. Yeah, I like this code more. The one catch with a project like this where we haven't done anything with it yet is that doesn't have code to reference for how it should format things. Like, I don't love the string formatting it's doing here. But, this is totally workable. I do want to make the string formatting less annoying, though. So, I'm going to ask it to do that. Can we use dent or something similar to make the string formatting less gross? And again, this is where being a real engineer is useful. I'm seeing formatting stuff that is obnoxious. I know a path to fix it. So, I'm recommending the path to fix it. And it's really cool. You can do these in the separate work trees because these are separate projects effectively. Let me switch over to this version and give it a shot. Copy my prompt quick. I'm just going to throw this in an empty thing here so I have it quick. Uh not there. Command option S. Here we are. Um I'll just put it here. Test prompt MD. Paste. Well, now I have that to grab whenever I want it. Bun run index. Oh, I forgot I was scrolled up. Bun run index. paste the topic. Oh, I forgot to choose the different models. Um, let me switch the models over quick, too. This is the one annoying thing about hopping between different things is if you make changes in one and you didn't make them on the other, you need to go back and handle that. The easiest thing I can do here rather than go find the specific code is just yoink these. Here's the models I want to use. Command shiftV for pasting without it linking to a file because I don't want to link from one work tree to the other. I just want the text content. Cool. Now that is done. Go back to the editor to copy my test prompt again. Paste. Send. Open this here so that I could take a look at the results. Oh, I think it hit one of the slow Kimmy K2 providers. What the hell did it do with all this empty output? That's annoying. I'll ask it to fix that. But let's take a look at the runs. Here's the original and screwed up the formatting. Man, I hate that I have to say this, but I think I'm happier with the version that was created by Composer than Opus cuz like the this is [ __ ] up the formatting a ton. Even here, um, I'll tell it that we'll give it one last chance to fix things. The indentation in the markdown files is all wonky. I screenshotted some examples. Please fix it. Let that fix it. But honestly, I'm leaning towards the composer version unless the quality of the essay is significantly better due to the system prompt difference. Yeah, I don't love the difference here. I think I'm going to stick with the version that I got from Composer. Honestly, it might have been a mistake. Honestly, now I think about it because of how we're writing this all out. That's annoying. String formatting is the most annoying thing in the [ __ ] world and no AI is smart enough to fix all that sadly. I'm going to rock the composer version. So now we can actually review it. I know I've read the code. I'm pretty happy with it. So I don't care too too much. But we're going to take a quick look. One last check. Yeah, I'm happy with it. We'll accept it. So now that I've accepted this, it just pulled all those changes in to the main branch in my main project folder here. So if I get diff, you'll see all those changes are now here. I'm going to do one last thing here that I think is almost always best practice. We're going to create a GitHub repo. I don't have the GitHub CLI installed. I have a lot of feelings about the GitHub CLI, but I'll try to use it at least. Push existing local repo. It is this one. Autodraftify. What do you mean it's not authenticated? You have my SSH key, [ __ ] Sorry. GH off login. Greatest piece of software. Oh, GitHub CLI. It doesn't appear to be smart enough to be done now that I did that. The GitHub CLI is legitimately one of the worst pieces of software I've ever [ __ ] used. Holy [ __ ] it's so bad. Apparently, you can't click the link. You have to press enter. So, here I can click that link, but if I do that, it won't work. I have to press enter and go through this way instead. I I made a video a while back about how the GitHub CLI is the most cursed pile of [ __ ] ever, and I got flamed for it in the first hour, so I took it down cuz I felt bad. I stand by every word. This is a tragedy. Not a piece of software. It has never done what it should ever once. Let let's let's do a quick poll in chat. Not a traditional poll. Just like we're going to have field drop numbers. How many flags or input fields do you think I'm going to have to fill out when I try to push this repo up using the CLI? How many steps do you think it'll take? And bonus points. how many of them are going to be blocking on a network request where I have to sit there and wait for the network request to go back and forth and specify both. So say uh inputs colon number blocking colon other number seven and two 9 and five five steps two blocks 6 plus two network y'all ready for this? So first one I have to do push existing repo I'm in a repo that's never been pushed that has no origin. You should be able to infer that path of local repo. I'm in the repo. Cool. It got the name right from the folder. It did one thing right so far. That was the first network block. It just network blocked there. Pick the project like the repository owner. That's the first one that made sense. Description. Why the [ __ ] do I need to write a description right now? Who writes a description before they put up the repo? When you go click the create button on the website, it doesn't make you put in a description. Skip. Polic internal. Another one that actually makes some sense. Cool. Cool. So, we've had two that make sense so far. That was a network block. Add a remote. No, I already told you I want the local repo. Why would I not want the remote? In what world would I say no here? Why would I ever? What should the new remote be called? I don't know. What do you think it should be called, GitHub? It's like every single flag on the API for this is exposed over the UI. Here we're at 1 2 3 4 5 6 7 8 nine questions so far. Would I like to push the commits to the current branch to origin? Yeah. Step 11. 11 steps and three network blocks to push a repo up to GitHub. What the [ __ ] Anyways, pro tip on this. Actually, I'll show you in a second. Um, now this is pushed. get checkout-b Theo slash uh I'll just call this composer version get add-p I like -p a lot when I'm doing git additions because it's like a way to mini review changes as you go it doesn't add new files though so I'm just going to do dash a for that d-m composer implementation of autodraftify and now I'm going to push I already have my git config locally so if I push on a branch that isn't there yet it'll create the origin makes my life a lot easier. So, it created the remote because it didn't exist yet. I can click this link to autocreate, but GHPR create seems like a great idea, right? Except it then asks another 15 [ __ ] questions. And this is the best part. If the names of your commits are too long, it'll make a mal formatted URL that doesn't work. So, the way I make a PR, I'll show you. I use Lazy Git. I press three. I press O. literally 100 times faster. It's so much better. It's so much better. I I I don't even use any of the features in Lazy Git. I literally just use it as a way to open a PR faster. LG30. It's like embedded in my brain now. So, now we have this PR. Let's create it. And now for the next thing I like to use AI for. Reviews. There's a lot of options for this. And honestly, I don't care which you use. Two of them sponsor me. Most of them are good. I like Code Rabbit and I really like Reptile, too. They're both great options. All of them are fine, though. Grapile is the easiest for me to quickly set up in a new open source repo. So, I'm going to go set up grapile really quick. So, here I have grap tiles dashboard settings enabled. Let's add new one. Enable. I don't why there's two enable buttons. What are you guys doing? Someone was vibe coding a little too close to the sun. One sec to send that to them. Well, now we have this enabled on autodraftify. It's going to index. It shouldn't take too long to index because there's almost no code in there. But now that we've done that, I can add grapile review, please. And see that eyes emoji. I love this. This is how you know Grapile is doing its job. It is starting to do its review. So, in a little bit, Graile will have it review of this code. I'm not going to wait for that just yet, though. It is really nice to have another AI, another tool like this reviewing the changes just to do a quick pass on it. There are actually features in cursor to have an AI do the review in the editor. I don't like them. I like doing code reviews on a code review platform like Graphite or in this case, GitHub using Grapile. Lots of options. I don't care which. But I do really like PRs as ways to group the work you did and have another tool take one last look before you hit merge. To show you an example of how this looks, I'll show you some PRs I have open on SnitchBench right now. Here is a couple of the ASDKV5 migrations. This is one of my favorite tests for new models and Opus oneshotted it and did a pretty good job. Let's take a look at the gravile review here. Confidence score is two out of five. Pay close attention to index. The tool result extraction logic needs immediate correction before merging. Very interesting. Apparently, ASKV step the tool results returns objects with a result property, not output. This will cause the tool results to be undefined. I can commit the suggestion directly from here or copy this prompt, paste it into my editor and have it make the change there. That's a very good catch though. I'm very thankful that I ran this. See how useful these tools are. You get the idea. Very nice to have those types of things. But I've accepted this. I'm happy with the state of things. Let's keep grinding. So, all the tasks we've done so far have been relatively simple, but this workflow can work for more complex things, too. The key is to pay attention to what the models are struggling with. So here is my snitchbench benchmark and as I was showing here getting it to upgrade to the latest AI SDK was not trivial and something I've noticed as I have tried this many times first off is that there was a lot of things that were broken in cursors workree implementation that are now mostly fixed but the other thing I noticed was that the models would struggle a lot because they couldn't really run the project in a way to get good feedback. They would often try to run the whole project. And this is a benchmark that takes six to eight hours to run. So, it wouldn't actually know if the results were any good. So, I knew this would be a lot better if I could tighten the feedback loop. Models work way better when you give them the pieces they need to understand when things are and aren't working. If you can give the model the tools it needs to recognize something isn't working and then fix it, it's way fewer steps that you need to be involved in. You can let it go off, run again and again until it gets things working and then go take a look after. And I noticed a massive improvement in how reliably this set of tools could make changes to this project when I gave it some really basic tests. Not like unit tests, although those aren't a bad idea either. More implementation testing environment stuff. So here is an example of how I did all of this. Back when I was still on the V4 SDK, I wanted an easier test bed. So, I wrote this prompt. I want to make two significant changes to the project. First, I want tests to also define the set of prompts that they include. Instead of blindly reading all of the MD files from the test directory because previously this project had one folder named prompts and it would just read through all of them in alphabetical order, I want to split it up so I can have two versions. One that is doing tool calling with just basic math functions and one that's the real benchmark. But I also want to create a simple tool call test that we can use. I also said I want a dry run mode that will be run with this new simple tool call test because if I add these two things now I have the ability to tell the model, hey go run dry run, take a look at the outputs and see if it worked or not. And this was a massive improvement to the quality of the outputs. I also noticed that the results were often not type safe which was very annoying for me. So I added a one last piece as well. I think I merged this in. I did. So let's look at my recent commits. So first I did the dry run so that it would have the ability to run on cheaper models to get some feedback. But even the cheap models were taking forever. So I made additional changes that aren't in this branch. I'll just make a PR for it quick because I need to do this anyways. The updated runs. Create PR latest AI SDK fixes for reasoning stuff. basic math test for dry runs, type checking, etc. Now, I have this PR up, but the pieces I wanted to emphasize in here that have made things significantly better are uh package.json. Here I added the dry run command and implemented the simpler test that would get feedback faster. I also have tsc as a command here too. TSC is bunx TSC- noit. So this is a command to check the type safety of the project cuz I noticed a lot of models were generating code that wasn't type safe and I wanted to be able to tell the model, hey, make sure the results are type safe. I also updated the prompt that I use for this. I do actually have it saved because I use this as a test a lot. Obviously, you're not going to be saving a prompt that you're firing one time to revisit later on, but I use this as a model test, so I rewrote the prompt accordingly. I used to only use these first two pieces here. Upgrade the project to use the latest version of the ASDK version 5. I've attached a guide on how to do the upgrade below. Please make sure to review the guide before starting your implementation with this link to the official migration guide from AISDK from Verscell. Make sure that you use the latest version of the AISDK package as well as Zod and the open router provider. This is meant to be a hint cuz I noticed a lot of the time they weren't updating the other packages, so I just explicitly told it to. Test your implementation by running the following command. And this is where all of the new stuff was added. And these pieces here massively improved the likelihood that I get code out that works. I gave it the bun run dry run command. Each test should have a result written to the results dryrun directory. Make sure you validate these outputs and that the tool calls that they have to do include the parameters that were passed by the model. This is again I added this cuz I noticed that the generation wasn't putting this in properly like the markdown coming out didn't include the input. So I told it to fix that. I've had problems where the output ends up being empty objects instead of the expected output of what the model submitted in the tool call. I call out what went wrong in my prompt. Sometimes it's easier to go back and rewrite the original prompt with more context and more guardrails and a better harness than it is to tell the model to fix things. I often find like if the results just not working how you behave half or more the time it's better to restart update your prompt embed the learnings and if you can write tests or calls that it can do to verify it's not making the same mistake just like a junior engineer you need to give the AI models and the agents that you're working with the necessary tools and resources to work through the mistakes it might make so if you notice it making a mistake maybe go reset call out the mistake and maybe add some harnesses so that it can check these things before committing the changes. Because of the types being wrong, I added this piece here. Make sure all the TypeScript in the project is validated. We don't want to be shipping any unsafe code in this project. Please avoid using any and any other things that might break the type safety of the system. Make sure to validate the changes that you make using the bun runtsc command. This is an example of how I made this project behave way better by slightly changing the prompting. It is worth noting that more of the pieces here are me giving it harnesses than me telling it what I actually want. But that's fine. I honestly think a 50-50 ratio like this in a prompt for a complex enough project is fair. But the biggest learning here by far for me at least was how much smarter the models get when you give them these little nuggets. Once I created the dry run test, it was way cheaper to run. It was way more reliable and I got results that were what significantly closer to what I was expecting. I am curious how this review is going if uh Grapile is touching it. Is it? It did. Got a three out of five. Apparently, it's upset with something I did in the index.ts. Let's take a look at that. Well, another fun example here. This is when I start using the editor stuff like in the editor mode. I thought this would be a quick fix. I was going to go change it, but the timeout ID might not exist because of how let definitions work in Typescript. Very annoying. So, I know this code is fine, but we're getting a type error. So, I'm going to go to this line, press command K, and say, I'm getting a type error with this check. Please fix it. Let's see what it does. Still broken because it's not instantiated. It's not smart enough to do that. So, we will reject and pull in a smarter model. This is one of the times where I would still use the editor's like agent mode here. Command N for a new piece here. I don't trust composer for something like this. So, I'm just going to use opus because it's the closest one for me to click. Timeout ID is showing a type error in the finally block due to potentially not being instantiated. Fix it, please. Okay, let's see the change it made. It made change 306. It made the type of timeout ID return type or undefined. So now we're not getting that error case because this is this is a fun little typescript thing actually says we define let timeout ID with this specific type but there are conditions where hypothetically this might not be set. So we don't set it immediately. The type for timeout ID was no.js timeout but we don't know if it's instantiated or not. So the two types it could be are the right type or uninstantiated. By doing or undefined here, we are telling Typescript it's okay for us to touch this even if it doesn't exist because the non-existing case is now something we've encoded. Cool. I didn't really think that would be the solution, but I learned something. And that's again the cool stuff with these tools. I had other ways to solve this. I personally wouldn't have used let but to each their own. Now I learned something and made an improvement. I even bet I can delete those. And this is fine. Let's keep those changes. And now the non-null assertion is gone. Remove nonnull assertions. Get push. And now Grapile can do an updated review. And now we are golden. Seven files reviewed, no comments. Did it update their review earlier? I bet they did. PR safe emerge with minimal risk. index should be monitored for the custom tool loop behavior with various providers, especially the thinking block handling logic. That's because of a change anthropic made to how it persists thinking blocks that broke a ton of [ __ ] It works, but yeah, Michaela asking, I mentioned coming around on planning a little bit ago in tweets. How good is it really? It's significantly better because you can use a smart model to write a thorough plan and it's not going to get lost in the weeds of like going down weird tanges writing the code. it can write the plan and then you can have a dumb model, go implement it, notice anything that's wrong, and then go back and adjust the plan. Actually, that's a good tangent I should go on here. So, earlier I was showing you guys this improved prompt that I wrote to try and get better results when doing harder tasks. This is another part of why I think planning is actually really good and underrated for anything even somewhat vaguely complex. When you write a prompt like this and you tell it to go write a plan based on this prompt, then you take that plan and run a dumb model against it and it makes mistakes. You can look at those mistakes and then tell the planning model to go update the plan based on those mistakes and then rerun. One of the important things to like get into your head when you're building this way is that the code is cheaper now. Like way cheaper. You shouldn't get fixated on a specific branch you're going down with the agents because you can just throw that away. It's silly, but when you're doing these types of like higher level architectural changes using agents, treat the prompt as the thing that you're maintaining between attempts, not the code. Because if you build this plan and run it against a dumb model and it makes two dumb mistakes, the code it just generated was really cheap and it did it really fast. You don't need to try and force it to fix that code when you can just throw it away. Tell the planning model to change the plan very slightly and then rerun it with the dumb model and regenerate the whole thing. It's a lot easier to keep it from going down these weird paths if you just throw out the weird path and try again. I would say at this point when I'm doing these heavier tasks, for every like one big change I hit accept in merge, there were like four or five attempts that made me go back to the original prompt or the original plan to adjust. That said, there are some higher level things you can do to make it more likely to be successful on that first attempt. One that I haven't tried just yet personally, but I've heard really good things about from Ben is cloning the repos for the projects that you're building around. So, if you're using effect heavily, effects is a it's a language. It's called a package, but it really is a different language built into Typescript, it takes a bit to get used to, and their docs kind of suck. The onboarding is awful, and there's a lot of functionality that's just not included in here. So, if you want to take more advantage of effect and make sure you're using the right pieces, telling it to go index the docs isn't going to get you very far. I've had awful luck with tools like context 7 is just not good enough. Supposedly, you can clone the effect repo as a subree, give it to Claude, and tell it to use that repo as docs. In particular, the tests in that repo, the tests directory or the test files in a given project end up being really useful to the models for these things. because it's full of good examples. Models and agents do significantly better given a handful of specific things. The first, which we just talked about, is a plan. A prompt is not good enough. A prompt can be used to write a plan. And once you verify the plan, the plan could be used for the implementation. Another really useful thing is a verification harness, some method it can use to know things work. This could be tests it can run a simpler version of the app it can run type checking and specific commands and scripts you give it in something like JavaScript you can in the package JSON write custom scripts it can run those types of things that put output and standard out that it can read and see and learn from not learn like get smarter but learn like oh I made this mistake I should go fix it really helping reduce that cycle and that loop of trying things and fixing them one of the biggest by far context and I don't just mean context on all the things in your project. I mean context in the things it has access to availability to and examples for how it should do those things. I've actually found that a lot of agents get better in big projects because once they're indexed, it has lots of examples of good patterns and also probably some examples of bad patterns that it can use in reference when it is developing and iterating. Having those examples is really really helpful. And if you write a lot of projects using the same few packages, it's not the worst idea to yoink some of that code over to point at and be like, \"Hey, here's how I like to do things.\" If you can paste one or two examples in your prompt, good chance it'll increase the quality of your output. But once the codebase has enough things or god forbid you write some tests to have this context in your codebase, quality of the outputs you're going to get, it's going to go up a ton. If you want to try the embedding the repost thing out, Ben, my channel manager, who also has a YouTube channel, you should check out his channel by the way, Faze, show it on the screen and I'm sure we'll have a link in the description. He's talking a lot about this stuff and he's coding way more than I am right now. He threw up a repo for this for those who want to try it out themselves. Works with cursor and open code. I'm personally finding myself in cursor much more than any of these CLI tools. The CLI tools just don't have a good enough workflow for when you're changing things or want to edit things or do things. but also they don't have stuff like LSP support, so they don't know when the types are wrong unless you give them a command they can run, which is super annoying. I really like having the integration between my editor and my agent flow. And I really like the agents view when I'm doing this. I almost treat the editor view like a like a review view where I'm in here to look at the code and if the models are just not doing things right, I'll sit here and code it out. Sometimes I need to just go write the index ts or write out exactly how I want to call things. I'll often pretend that the API I want exists and write out the code and leave all the type errors in for how I want to work on a thing or how I want it to behave. I'll just write the exact code I want knowing nothing works yet and then tell the models or tell the agent make me a plan that will fulfill this functionality so this behaves. And then I go through my usual loop. And again, my loop is make a prompt, create a plan, edit the plan, run the plan, and if the plan doesn't work, closely inspect the run, see what failed, what it got wrong, why it got it wrong, go back, adjust the plan accordingly, and rerun. And I go through this again and again. Or, and this is where you really need to like get a good gut feel and taste. If it seems like it's just so far off from what you're trying to get it to do, you need more information, more context, more harness for it. At that point, go write some tests that can execute, write an example file that shows what you're trying to do, then maybe try the agent again or just go finish writing the code. It's a lot easier sometimes to just go do it. And I still find myself doing that here and there. A lot of it's just gut feel. Like you got to recognize like this is going to take too much work to make the model behave. it's better if I just go do it myself. But sometimes it's worth pushing through that because you might be surprised. You might learn more of these prompting skills and general like agentic flow things and get better. Or you might just have a really funny thing to tweet about how stupid AI models are. Either way, it's pretty beneficial to go down these rabbit holes at times. But don't be scared to move off it. And I know I've emphasized this a lot in other videos, but I don't know if I have here yet, so I'm going to triple down on this point. Do not use these models to build things you don't understand. These agents are great for doing a lot of work that you already know how to do but don't feel like doing. I find myself spinning up one-off projects and sandboxes for experiments way more. I find myself writing tests and example cases way more. I find myself writing formalized plans way more. I'm doing these things because it's way faster and easier than it's ever been. And the result is better code than I would have written by hand or code being written when I would have just moved on from this project. Like I'm gonna go play a ton with this autodraftify thing when I'm done streaming. This is an idea I had last night when I was falling asleep. I wrote it down on my phone and then I just went and coded it in what, like 10 minutes. That's so cool. I've always been a fast developer, but previously this would have taken me like 30 minutes to an hour and I would have had to be sitting there focusing the whole time. Now I can do it in five to 10 while working on other stuff in the background and playing with all these fun tools. That's awesome. I feel like I'm developing a whole new set of skills and there's a good chance that my take on all of these things is going to change a ton even by the time the video is out. But this is a rough idea of the workflow I have found myself using when I play with these things and when I build stuff with it. And I found it to just be awesome. Also, believe it or not, I know Curse is a sponsor. Call me a shell. Call me whatever. I am still on the $20 a month tier. I have never had to pay an overage fee and I'm doing this [ __ ] a ton. It's not super expensive to do all these things. I know there's a stereotype of like the AI agents are so expensive, why not just go do it yourself? If you use the expensive ones for something relatively cheap and low on token utilization like planning, then you use a cheap fast model like a grot code, composer, whatever else to do the implementation. It ends up not costing that much at all. I'm still very much in the green on my cursor usage for my current subscription. And I know the majority of people I work with have not even gotten close to hitting those limits. So yeah, 20 bucks a month to fundamentally change how I wrote code. For me, that's really worth it. As a person who was super skeptical of cursor and was scared to invest initially because didn't think it was going to be for me. Now I love this team. I love this editor. It's how I prefer to work. And while I do enjoy experimenting with new tools and never shy away from trying something if it's interesting to you, like if you're curious how Gemini CLI works, go give it a shot. If you're curious why everybody's so excited about Cloud Code, go try it. If you're curious about what's going on with Codeex and why they rewrote in Rust, I can't give you a good answer, but I can tell you to go try it. But if you're not interested in those things, don't feel like you have to. It's totally okay to get to the stuff a bit late. I think we're now at the point where most developers should start playing with agents in figuring out what their agentic workflow looks like. But you shouldn't feel like you have to go learn all of these tools, you're going to get fired. You should take this opportunity to try these things out. See what they are good at. See what they're bad at. And one last spicy take, getting better at this stuff will make you a better coworker. Because getting better at clearly giving instructions to your tools and getting better at giving clear reproduction steps, things like tests and harnesses to the agents to write this code. Once you get good at doing that, you've improved your communication skills massively. And now when you work with co-workers or if you become a manager and have your own employees on your own team, your ability to communicate with them will go up too. This is a stress test on how clear and concise you can be as you build. This is also why I hate all those MCPs that just fill up your context window with a bunch of garbage documentation. No, [ __ ] that. Get better at writing. Get better at clarifying what you want and get better at writing the necessary tools that allow for the agents to know if they succeeded or failed at their goals. I have been loving this. I've genuinely had so much fun building in this way. I feel rejuvenated writing code in a way I haven't in a long time. And while there are still lots of tasks that I find the agents just aren't that great at, especially like big overhauls using underdocumented technologies, man, it's so fun to play with this stuff. Highly recommend giving it a shot if you haven't just yet. Let me know what you guys think. Am I a crazy AI pill vibe coding lunatic or is this actually useful? Let me know in your own workflows and tell me how you've been coding with AI in the comments. And until next time, keep coding."
}